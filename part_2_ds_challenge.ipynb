{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e6bfb6",
   "metadata": {},
   "source": [
    "# 0. Instructions and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc153a5d",
   "metadata": {},
   "source": [
    "## 0.1. Instructions. Part 2: Data Scientist Challenge (3.5 points)\n",
    "\n",
    "- **Objective:** Explore different techniques to enhance model performance with limited  labeled data. You will be limited to 32 labeled examples in your task.  The rest can be viewed as unlabelled data. \n",
    "\n",
    "- **Tasks:**\n",
    "  - **a. BERT Model with Limited Data (0.5 points):** Train a BERT-based model using only 32 labeled examples and assess its performance.\n",
    "  - **b. Dataset Augmentation (1 point):** Experiment with an automated technique to increase your dataset size **without using LLMs** (chatGPT / Mistral / Gemini / etc...). Evaluate the impact on model performance.\n",
    "  - **c. Zero-Shot Learning with LLM (0.5 points):** Apply a LLM (chatGPT/Claude/Mistral/Gemini/...) in a zero-shot learning setup. Document the performance.\n",
    "  - **d. Data Generation with LLM (1 point):** Use a LLM (chatGPT/Claude/Mistral/Gemini/...) to generate new, labeled  dataset points. Train your BERT model with it + the 32 labels. Analyze  how this impacts model metrics.\n",
    "  - **e. Optimal Technique Application (0.5 points):** Based on the previous experiments, apply the most effective  technique(s) to further improve your model's performance. Comment your results and propose improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d48a5",
   "metadata": {},
   "source": [
    "## 0.2. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f98abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Using cached polars-1.30.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Using cached polars-1.30.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-1.30.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install polars  # Install polars for faster data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81e6d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from library.metrics import Metrics\n",
    "from library.utilities import set_seed\n",
    "from datasets import Dataset\n",
    "\n",
    "# Deep Learning and NLP\n",
    "import torch\n",
    "from setfit import SetFitModel, Trainer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570741d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the metrics object to save the results\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f7babdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check the availability of a GPU\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280eac7",
   "metadata": {},
   "source": [
    "## 0.3. Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53a2aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42. This ensures reproducibility of results across runs.\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b700f",
   "metadata": {},
   "source": [
    "## 0.4. Loading the data: Swiss Judgement Prediction\n",
    "\n",
    "Source: https://huggingface.co/datasets/rcds/swiss_judgment_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87ea67e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded DataFrame shape: (35386, 9)\n",
      "\n",
      "Loaded DataFrame schema:\n",
      "Schema({'id': Int32, 'year': Int32, 'text': String, 'label': Int64, 'language': String, 'region': String, 'canton': String, 'legal area': String, 'split': String})\n",
      "\n",
      "First few rows of the loaded DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>year</th><th>text</th><th>label</th><th>language</th><th>region</th><th>canton</th><th>legal area</th><th>split</th></tr><tr><td>i32</td><td>i32</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>22014</td><td>2011</td><td>&quot;Faits: A. Le 28 octobre 2002 à…</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;ge&quot;</td><td>&quot;civil law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>11593</td><td>2007</td><td>&quot;Faits : Faits : A. Le 17 avril…</td><td>1</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;ge&quot;</td><td>&quot;penal law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>26670</td><td>2013</td><td>&quot;Faits: A. Par jugement du 2 ma…</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;vd&quot;</td><td>&quot;penal law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>5864</td><td>2004</td><td>&quot;Faits: Faits: A. N._, née en 1…</td><td>1</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;vd&quot;</td><td>&quot;insurance law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>16122</td><td>2009</td><td>&quot;Faits: A. Y._ est propriétaire…</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;ge&quot;</td><td>&quot;public law&quot;</td><td>&quot;train&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌───────┬──────┬───────────────────┬───────┬───┬──────────────────┬────────┬───────────────┬───────┐\n",
       "│ id    ┆ year ┆ text              ┆ label ┆ … ┆ region           ┆ canton ┆ legal area    ┆ split │\n",
       "│ ---   ┆ ---  ┆ ---               ┆ ---   ┆   ┆ ---              ┆ ---    ┆ ---           ┆ ---   │\n",
       "│ i32   ┆ i32  ┆ str               ┆ i64   ┆   ┆ str              ┆ str    ┆ str           ┆ str   │\n",
       "╞═══════╪══════╪═══════════════════╪═══════╪═══╪══════════════════╪════════╪═══════════════╪═══════╡\n",
       "│ 22014 ┆ 2011 ┆ Faits: A. Le 28   ┆ 0     ┆ … ┆ Région lémanique ┆ ge     ┆ civil law     ┆ train │\n",
       "│       ┆      ┆ octobre 2002 à…   ┆       ┆   ┆                  ┆        ┆               ┆       │\n",
       "│ 11593 ┆ 2007 ┆ Faits : Faits :   ┆ 1     ┆ … ┆ Région lémanique ┆ ge     ┆ penal law     ┆ train │\n",
       "│       ┆      ┆ A. Le 17 avril…   ┆       ┆   ┆                  ┆        ┆               ┆       │\n",
       "│ 26670 ┆ 2013 ┆ Faits: A. Par     ┆ 0     ┆ … ┆ Région lémanique ┆ vd     ┆ penal law     ┆ train │\n",
       "│       ┆      ┆ jugement du 2 ma… ┆       ┆   ┆                  ┆        ┆               ┆       │\n",
       "│ 5864  ┆ 2004 ┆ Faits: Faits: A.  ┆ 1     ┆ … ┆ Région lémanique ┆ vd     ┆ insurance law ┆ train │\n",
       "│       ┆      ┆ N._, née en 1…    ┆       ┆   ┆                  ┆        ┆               ┆       │\n",
       "│ 16122 ┆ 2009 ┆ Faits: A. Y._ est ┆ 0     ┆ … ┆ Région lémanique ┆ ge     ┆ public law    ┆ train │\n",
       "│       ┆      ┆ propriétaire…     ┆       ┆   ┆                  ┆        ┆               ┆       │\n",
       "└───────┴──────┴───────────────────┴───────┴───┴──────────────────┴────────┴───────────────┴───────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned Parquet file\n",
    "df = pl.read_parquet('swiss_judgment_prediction_fr&it_clean.parquet')\n",
    "\n",
    "# Display the loaded DataFrame\n",
    "print(\"\\nLoaded DataFrame shape:\", df.shape)\n",
    "print(\"\\nLoaded DataFrame schema:\")\n",
    "print(df.schema)\n",
    "print(\"\\nFirst few rows of the loaded DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f926bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into training, validation and test sets\n",
    "train_df = df.filter(pl.col('split') == 'train')\n",
    "val_df = df.filter(pl.col('split') == 'val')\n",
    "test_df = df.filter(pl.col('split') == 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c4b4b",
   "metadata": {},
   "source": [
    "# 1. BERT Model with Limited Data\n",
    "\n",
    "Outline of the intermediate tasks:\n",
    "\n",
    "1. Preprocessing Pipeline\n",
    "   - Lowercasing, punctuation stripping (or not, depending on BERT tokenizer).\n",
    "   - Sentencepiece/BPE tokenization via the CamemBERT (for French) or UmBERTo (for Italian).\n",
    "   - (Optional) language tags if you merge FR+IT in one model.\n",
    "2. Hold-out Split. Since you only have 32 labels: use stratified k-fold CV (e.g. 8 × 4-fold) to get reliable estimates, or leave-one-out if you want maximum training data per fold.\n",
    "3. BERT Model with Only 32 Examples\n",
    "   - Model Choice: Pick a multilingual BERT (mBERT) or separate CamemBERT/UmBERTo checkpoint. Alternatives:\n",
    "     - Multilingual/monolingual models:\n",
    "       -  [BERT multilingual base model (cased)](https://huggingface.co/google-bert/bert-base-multilingual-cased). [Uncased model](https://huggingface.co/google-bert/bert-base-multilingual-uncased) also available. Paper: \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\". \n",
    "       -  [CamemBERT 2.0](https://huggingface.co/almanach/camembertv2-base) and [CamemBERTav2](https://huggingface.co/almanach/camembertav2-base), models trained with French text and explained in the paper: [CamemBERT 2.0: A Smarter French Language Model Aged to Perfection](https://arxiv.org/html/2411.08868v1#S3). These models supposedly improve over the performance of the original [CamemBERT](https://huggingface.co/docs/transformers/en/model_doc/camembert) model, explained in \"[CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)\".\n",
    "       -  [FlauBERT](https://huggingface.co/docs/transformers/en/model_doc/flaubert), another model pre-trained on French text. Paper: \"[FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372)\". \n",
    "       -  [BERT Base Italian Cased](https://huggingface.co/dbmdz/bert-base-italian-cased), [Uncased](https://huggingface.co/dbmdz/bert-base-italian-uncased), ,and [XXL Uncased](https://huggingface.co/dbmdz/bert-base-italian-xxl-uncased). \n",
    "       -  [UmBERTo Commoncrawl Cased](https://huggingface.co/Musixmatch/umberto-commoncrawl-cased-v1), another model trained with a large corpus of texts in Italian.\n",
    "      -  Domain-specific models (law):\n",
    "          - [LEGAL-BERT](https://huggingface.co/nlpaueb/legal-bert-base-uncased) does not seem to be a good option as it was trained only on English data.\n",
    "          - [JuriBERT](https://huggingface.co/dascim/juribert-base) for legal texts in French. Paper explaining the model: [JuriBERT: A Masked-Language Model Adaptation for French Legal Text](https://arxiv.org/pdf/2110.01485).\n",
    "          - [ITALIAN-LEGAL-BERT](https://huggingface.co/dlicari/Italian-Legal-BERT) for legal text in Italian. Paper explaining the model: [ITALIAN-LEGAL-BERT models for improving natural language processing tasks in the Italian legal domain](https://www.sciencedirect.com/science/article/pii/S0267364923001188).\n",
    "     - Models to try: \n",
    "       - For both languages: BERT multilingual base model (cased).\n",
    "       - For French: CamemBERT, FlauBERT, CamemBERTav2 (based on DebertaV2 architecture), JuriBERT.\n",
    "       - For Italian: BERT Base Italian Cased, UmBERTo Commoncrawl Cased, ITALIAN-LEGAL-BERT.\n",
    "   - Fine-tuning Setup.\n",
    "     - Freeze or unfreeze last n encoder layers—try both.\n",
    "     - Small learning rate (2e-5 – 5e-5), batch size = 8 or 16.\n",
    "     - Early stopping on validation loss.\n",
    "4. Training & Evaluation\n",
    "    - Run your k-fold CV training loops.\n",
    "    - Track accuracy, F1, precision, recall per fold.\n",
    "    - Report mean ± std.\n",
    "5. Error Analysis and feature interpretation.\n",
    "    - Use `LIME` for analyzing the most relevant features for classifying the texts. \n",
    "    - Look at which examples are mispredicted.\n",
    "    - Check language breakdown (FR vs. IT) to see if one is harder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831052f",
   "metadata": {},
   "source": [
    "## 1.1. Standard fine-tuning\n",
    "\n",
    "Notebook of reference: `Session_5_1_BERT_HF_Implementation.ipynb`, sections 1, 2 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e98272",
   "metadata": {},
   "source": [
    "## 1.2. Using SetFit (\"Sentence Transformer Fine-Tuning\")\n",
    "\n",
    "Notebook of reference: `Session_6_2_Zero_Shot_Classification.ipynb`, introduction, step 1 (loading data) and step 6 (\"Few-Shot Classification with SetFit\").\n",
    "\n",
    "Applying SetFit (the “Sentence Transformer Fine-Tuning” recipe) absolutely counts as **training** (it fine-tunes a pre-trained sentence-embedding model, plus fits a small classifier on top). And indeed, SetFit was built **for** the exactly your scenario—getting strong performance with as few as a few dozen labeled examples.\n",
    "\n",
    "---\n",
    "\n",
    "**Why SetFit = Training**\n",
    "\n",
    "* **Contrastive fine-tuning:**\n",
    "  You start with a frozen (or lightly unfrozen) Sentence-Transformer model and then *fine-tune* it on automatically generated sentence pairs derived from your 32 labels.\n",
    "* **Classifier head training:**\n",
    "  After contrastive tuning, you fit a lightweight logistic-regression (or small MLP) classifier on the resulting embeddings.\n",
    "* Both steps update model parameters—so it’s training/fine-tuning, not mere prompt-engineering or zero-shot.\n",
    "\n",
    "---\n",
    "\n",
    "**Why SetFit excels in limited-label regimes**\n",
    "\n",
    "1. **Data amplification via contrastive pairs**\n",
    "\n",
    "   * From each labeled example, SetFit creates positive pairs (e.g. two different augmentations of the same sentence) and negative pairs (across classes), turning 32 labels into hundreds or thousands of pairwise signals.\n",
    "   * That extra signal helps the embedding space separate classes, even when you only have a few “gold” labels.\n",
    "\n",
    "2. **Lightweight classifier**\n",
    "\n",
    "   * Because the embedding model has already been tuned to distinguish your classes, the final classifier can be a simple logistic or MLP—so it needs very few examples to learn decision boundaries.\n",
    "\n",
    "3. **Empirical few-shot strength**\n",
    "\n",
    "   * In benchmarks, SetFit often outperforms standard BERT fine-tuning when you have <100 labels, and it’s much faster to train (no full back-prop through all BERT layers).\n",
    "\n",
    "---\n",
    "\n",
    "**How to plug SetFit into the task**\n",
    "\n",
    "1. **Install** the SetFit library (e.g. via `pip install setfit`).\n",
    "2. **Initialize** a pre-trained checkpoint:\n",
    "\n",
    "   ```python\n",
    "   from setfit import SetFitModel, SetFitTrainer\n",
    "   model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "   ```\n",
    "3. **Prepare** your 32 labeled examples as `(text, label)` tuples.\n",
    "4. **Train** the model:\n",
    "\n",
    "   ```python\n",
    "   trainer = SetFitTrainer(\n",
    "       model=model,\n",
    "       train_dataset=my_32_examples,\n",
    "       eval_dataset=my_dev_split,\n",
    "       metric=\"accuracy\",\n",
    "       loss=\"cosine-similarity\",\n",
    "       batch_size=16,\n",
    "       num_iterations=20,          # controls number of contrastive steps\n",
    "       num_epochs=1                # just one pass for the classifier\n",
    "   )\n",
    "   trainer.train()\n",
    "   ```\n",
    "5. **Evaluate** on your held-out data.\n",
    "\n",
    "You’ll have fine-tuned embeddings *and* a classifier head—all with only 32 labels. That makes SetFit not just “considered training,” but *one of the best* training-with-few-labels recipes out there.\n",
    "\n",
    "---\n",
    "\n",
    "Note that there are 2 aspects that we need to implement for SetFit to work properly:\n",
    "1. **Balance the examples of positive and negative classes** that will be passed for contrastive learning. Since approximately 29% of the labels are positive (approved motions) and the other 71% correspond to rejected motions, passing examples for contrastive learning without balancing the clases would bias the embedding space towards the majority class.\n",
    "2. We need to **run several iterations** in order to get lower variability in the results, as a single training run can be highly variable due to randomness in the example sampling (as we only pass 32 labelled examples). Therefore, if we don't run several iterations for SetFit, the results would be highly noisy. Averaging across iterations smooths out this variance and gives us the true expected performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample size (labelled examples for training)\n",
    "sample_size = 32\n",
    "num_iterations = 10  # Run several iterations for the sample size (32 labels) to minimize the impact of randomness\n",
    "\n",
    "# Function to sample balanced dataset\n",
    "def sample_balanced_dataset(dataset: pl.DataFrame, num_samples, seed):\n",
    "\n",
    "    # Get positive and negative examples\n",
    "    pos_examples = dataset.filter(pl.col('label') == 1)\n",
    "    neg_examples = dataset.filter(pl.col('label') == 1)\n",
    "    \n",
    "    # Sample equal numbers from each class\n",
    "    samples_per_class = num_samples // 2\n",
    "    \n",
    "    if seed is not None:\n",
    "        pos_sampled = pos_examples.sample(n=samples_per_class, shuffle=True, seed=seed)\n",
    "        neg_sampled = neg_examples.sample(n=samples_per_class, shuffle=True, seed=seed)\n",
    "    else:\n",
    "        raise ValueError(\"Seed must be provided for reproducibility.\")\n",
    "    \n",
    "    # Concatenate the sampled DataFrames, only the text and the label columns\n",
    "    pos_sampled = pos_sampled.select(['text', 'label'])\n",
    "    neg_sampled = neg_sampled.select(['text', 'label'])\n",
    "    df = pl.concat([pos_sampled, neg_sampled], how='vertical')\n",
    "\n",
    "    # Combine the datasets into a single Dataset object\n",
    "    combined = Dataset.from_polars(df)\n",
    "    \n",
    "    return combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample sizes to experiment with\n",
    "sample_sizes = [4, 8, 16, 32, 64, 128]\n",
    "num_iterations = 10  # Run several iterations for the sample size (32 labels) to minimize the impact of randomness\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "results = {size: {metric: [] for metric in metrics} for size in sample_sizes}\n",
    "\n",
    "#dataset['train'] = dataset['train'].rename_column(\"labels\", \"label\")\n",
    "#dataset['test'] = dataset['test'].rename_column(\"labels\", \"label\")\n",
    "\n",
    "# Function to sample balanced dataset\n",
    "def sample_balanced_dataset(dataset, num_samples, seed=None):\n",
    "    # Get positive and negative examples\n",
    "    pos_examples = dataset.filter(lambda x: x['label'] == 1)\n",
    "    neg_examples = dataset.filter(lambda x: x['label'] == 0)\n",
    "    \n",
    "    # Sample equal numbers from each class\n",
    "    samples_per_class = num_samples // 2\n",
    "    \n",
    "    if seed is not None:\n",
    "        pos_sampled = pos_examples.shuffle(seed=seed).select(range(samples_per_class))\n",
    "        neg_sampled = neg_examples.shuffle(seed=seed).select(range(samples_per_class))\n",
    "    else:\n",
    "        pos_sampled = pos_examples.shuffle().select(range(samples_per_class))\n",
    "        neg_sampled = neg_examples.shuffle().select(range(samples_per_class))\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    combined = Dataset.from_dict({\n",
    "        'text': pos_sampled['text'] + neg_sampled['text'],\n",
    "        'label': pos_sampled['label'] + neg_sampled['label']\n",
    "    })\n",
    "    \n",
    "    return combined.shuffle(seed=seed) if seed is not None else combined.shuffle()\n",
    "\n",
    "# Prepare test set (use a fixed portion of the dataset)\n",
    "test_set = dataset['test'].select(random_indices)\n",
    "\n",
    "# Run experiments\n",
    "for size in sample_sizes:\n",
    "    print(f\"Training with {size} examples...\")\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"  Iteration {iteration+1}/{num_iterations}\")\n",
    "        \n",
    "        # Sample balanced training data\n",
    "        train_samples = sample_balanced_dataset(dataset['train'], size, seed=42+iteration)\n",
    "        model = SetFitModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Initialize and train SetFit model\n",
    "        trainer = SetFitTrainer(\n",
    "            model=model,\n",
    "            train_dataset=train_samples,\n",
    "            eval_dataset=test_set,\n",
    "            batch_size=16,\n",
    "            num_iterations=20,\n",
    "            num_epochs=1\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate\n",
    "        predictions = trainer.model.predict(test_set['text'])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[size]['accuracy'].append(accuracy_score(test_set['label'], predictions))\n",
    "        results[size]['f1'].append(f1_score(test_set['label'], predictions))\n",
    "        results[size]['precision'].append(precision_score(test_set['label'], predictions))\n",
    "        results[size]['recall'].append(recall_score(test_set['label'], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaca434",
   "metadata": {},
   "source": [
    "# 2. Dataset Augmentation\n",
    "\n",
    "Outline of the intermediate tasks: We want a fully automated pipeline. A good candidate is Easy Data Augmentation (EDA) or back-translation via open‐source MT models.\n",
    "1. Choose Technique(s)\n",
    "   - EDA: random synonym substitution (WordNet or fastText), random swap, insertion, deletion.\n",
    "   - Back-translation: FR → EN → FR and IT → EN → IT using MarianMT or opus-MT.\n",
    "2. Implement & Generate\n",
    "   - For each of the 32 labeled examples, generate k augmented pseudo-examples (e.g. k=5).\n",
    "   - Deduplicate and filter (e.g. reject if new text <50% overlap).\n",
    "3. Merge & Re-split\n",
    "   - Combine original 32 + synthetic N = 32×k examples.\n",
    "   - Re-run the same CV split strategy, ensuring augmented copies of a given original stay in the same fold.\n",
    "4. Re-train BERT\n",
    "   - Exactly the same hyperparams as in (a).\n",
    "   - Track performance uplift vs. the baseline.\n",
    "5. Analysis\n",
    "   - Compare metrics: ΔAccuracy, ΔF1.\n",
    "   - Ablation: EDA vs. back-translation vs. combined.\n",
    "   - Qualitative: inspect a few synthetic samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ccf8a",
   "metadata": {},
   "source": [
    "# 3. Zero-Shot Learning with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb57e3",
   "metadata": {},
   "source": [
    "# 4. Data Generation with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ad1cc",
   "metadata": {},
   "source": [
    "# 5. Optimal Technique Application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bse-nlp-YMIcxN07-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
