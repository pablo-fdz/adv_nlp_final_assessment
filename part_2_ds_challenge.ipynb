{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e6bfb6",
   "metadata": {},
   "source": [
    "# 0. Instructions and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc153a5d",
   "metadata": {},
   "source": [
    "## 0.1. Instructions. Part 2: Data Scientist Challenge (3.5 points)\n",
    "\n",
    "- **Objective:** Explore different techniques to enhance model performance with limited  labeled data. You will be limited to 32 labeled examples in your task.  The rest can be viewed as unlabelled data. \n",
    "\n",
    "- **Tasks:**\n",
    "  - **a. BERT Model with Limited Data (0.5 points):** Train a BERT-based model using only 32 labeled examples and assess its performance.\n",
    "  - **b. Dataset Augmentation (1 point):** Experiment with an automated technique to increase your dataset size **without using LLMs** (chatGPT / Mistral / Gemini / etc...). Evaluate the impact on model performance.\n",
    "  - **c. Zero-Shot Learning with LLM (0.5 points):** Apply a LLM (chatGPT/Claude/Mistral/Gemini/...) in a zero-shot learning setup. Document the performance.\n",
    "  - **d. Data Generation with LLM (1 point):** Use a LLM (chatGPT/Claude/Mistral/Gemini/...) to generate new, labeled  dataset points. Train your BERT model with it + the 32 labels. Analyze  how this impacts model metrics.\n",
    "  - **e. Optimal Technique Application (0.5 points):** Based on the previous experiments, apply the most effective  technique(s) to further improve your model's performance. Comment your results and propose improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d48a5",
   "metadata": {},
   "source": [
    "## 0.2. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f98abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Using cached polars-1.30.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Using cached polars-1.30.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-1.30.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install polars  # Install polars for faster data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e6d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "import polars as pl\n",
    "import random\n",
    "from library.metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "570741d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the metrics object to save the results\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280eac7",
   "metadata": {},
   "source": [
    "## 0.3. Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a2aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b700f",
   "metadata": {},
   "source": [
    "## 0.4. Loading the data: Swiss Judgement Prediction\n",
    "\n",
    "Source: https://huggingface.co/datasets/rcds/swiss_judgment_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ea67e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded DataFrame shape: (35386, 9)\n",
      "\n",
      "Loaded DataFrame schema:\n",
      "Schema({'id': Int32, 'year': Int32, 'text': String, 'label': Int64, 'language': String, 'region': String, 'canton': String, 'legal area': String, 'split': String})\n",
      "\n",
      "First few rows of the loaded DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>year</th><th>text</th><th>label</th><th>language</th><th>region</th><th>canton</th><th>legal area</th><th>split</th></tr><tr><td>i32</td><td>i32</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>2000</td><td>&quot;A.- Par contrat d&#x27;entreprise s…</td><td>0</td><td>&quot;fr&quot;</td><td>null</td><td>null</td><td>&quot;civil law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>1</td><td>2000</td><td>&quot;A.- Le 12 avril 1995, A._ a su…</td><td>0</td><td>&quot;fr&quot;</td><td>null</td><td>null</td><td>&quot;insurance law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>2</td><td>2000</td><td>&quot;A.- En février 1994, M._ a été…</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;ge&quot;</td><td>&quot;insurance law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>3</td><td>2000</td><td>&quot;A.- M._ a travaillé en qualité…</td><td>0</td><td>&quot;fr&quot;</td><td>null</td><td>null</td><td>&quot;insurance law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>6</td><td>2000</td><td>&quot;A.- Le 29 septembre 1997, X._ …</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Espace Mittelland&quot;</td><td>&quot;ne&quot;</td><td>&quot;penal law&quot;</td><td>&quot;train&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌─────┬──────┬────────────────────┬───────┬───┬───────────────────┬────────┬───────────────┬───────┐\n",
       "│ id  ┆ year ┆ text               ┆ label ┆ … ┆ region            ┆ canton ┆ legal area    ┆ split │\n",
       "│ --- ┆ ---  ┆ ---                ┆ ---   ┆   ┆ ---               ┆ ---    ┆ ---           ┆ ---   │\n",
       "│ i32 ┆ i32  ┆ str                ┆ i64   ┆   ┆ str               ┆ str    ┆ str           ┆ str   │\n",
       "╞═════╪══════╪════════════════════╪═══════╪═══╪═══════════════════╪════════╪═══════════════╪═══════╡\n",
       "│ 0   ┆ 2000 ┆ A.- Par contrat    ┆ 0     ┆ … ┆ null              ┆ null   ┆ civil law     ┆ train │\n",
       "│     ┆      ┆ d'entreprise s…    ┆       ┆   ┆                   ┆        ┆               ┆       │\n",
       "│ 1   ┆ 2000 ┆ A.- Le 12 avril    ┆ 0     ┆ … ┆ null              ┆ null   ┆ insurance law ┆ train │\n",
       "│     ┆      ┆ 1995, A._ a su…    ┆       ┆   ┆                   ┆        ┆               ┆       │\n",
       "│ 2   ┆ 2000 ┆ A.- En février     ┆ 0     ┆ … ┆ Région lémanique  ┆ ge     ┆ insurance law ┆ train │\n",
       "│     ┆      ┆ 1994, M._ a été…   ┆       ┆   ┆                   ┆        ┆               ┆       │\n",
       "│ 3   ┆ 2000 ┆ A.- M._ a          ┆ 0     ┆ … ┆ null              ┆ null   ┆ insurance law ┆ train │\n",
       "│     ┆      ┆ travaillé en       ┆       ┆   ┆                   ┆        ┆               ┆       │\n",
       "│     ┆      ┆ qualité…           ┆       ┆   ┆                   ┆        ┆               ┆       │\n",
       "│ 6   ┆ 2000 ┆ A.- Le 29          ┆ 0     ┆ … ┆ Espace Mittelland ┆ ne     ┆ penal law     ┆ train │\n",
       "│     ┆      ┆ septembre 1997,    ┆       ┆   ┆                   ┆        ┆               ┆       │\n",
       "│     ┆      ┆ X._ …              ┆       ┆   ┆                   ┆        ┆               ┆       │\n",
       "└─────┴──────┴────────────────────┴───────┴───┴───────────────────┴────────┴───────────────┴───────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned Parquet file\n",
    "df = pl.read_parquet('swiss_judgment_prediction_fr&it_clean.parquet')\n",
    "\n",
    "# Display the loaded DataFrame\n",
    "print(\"\\nLoaded DataFrame shape:\", df.shape)\n",
    "print(\"\\nLoaded DataFrame schema:\")\n",
    "print(df.schema)\n",
    "print(\"\\nFirst few rows of the loaded DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c4b4b",
   "metadata": {},
   "source": [
    "# 1. BERT Model with Limited Data\n",
    "\n",
    "Outline of the intermediate tasks:\n",
    "\n",
    "1. Preprocessing Pipeline\n",
    "   - Lowercasing, punctuation stripping (or not, depending on BERT tokenizer).\n",
    "   - Sentencepiece/BPE tokenization via the CamemBERT (for French) or UmBERTo (for Italian).\n",
    "   - (Optional) language tags if you merge FR+IT in one model.\n",
    "2. Hold-out Split. Since you only have 32 labels: use stratified k-fold CV (e.g. 8 × 4-fold) to get reliable estimates, or leave-one-out if you want maximum training data per fold.\n",
    "3. BERT Model with Only 32 Examples\n",
    "   - Model Choice: Pick a multilingual BERT (mBERT) or separate CamemBERT/UmBERTo checkpoint. Alternatives:\n",
    "     - Multilingual/monolingual models:\n",
    "       -  [BERT multilingual base model (cased)](https://huggingface.co/google-bert/bert-base-multilingual-cased). Paper: \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\". \n",
    "       -  [CamemBERT 2.0](https://huggingface.co/almanach/camembertv2-base) and [CamemBERTav2](https://huggingface.co/almanach/camembertav2-base), models trained with French text and explained in the paper: [CamemBERT 2.0: A Smarter French Language Model Aged to Perfection](https://arxiv.org/html/2411.08868v1#S3). These models supposedly improve over the performance of the original [CamemBERT](https://huggingface.co/docs/transformers/en/model_doc/camembert) model, explained in \"[CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)\".\n",
    "       -  [FlauBERT](https://huggingface.co/docs/transformers/en/model_doc/flaubert), another model pre-trained on French text. Paper: \"[FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372)\". \n",
    "       -  [BERT Base Italian Uncased](https://huggingface.co/dbmdz/bert-base-italian-uncased), [Cased](https://huggingface.co/dbmdz/bert-base-italian-cased),and [XXL Uncased](https://huggingface.co/dbmdz/bert-base-italian-xxl-uncased). \n",
    "       -  [UmBERTo Commoncrawl Cased](https://huggingface.co/Musixmatch/umberto-commoncrawl-cased-v1), another model trained with a large corpus of texts in Italian.\n",
    "      -  Domain-specific models (law):\n",
    "          - [LEGAL-BERT](https://huggingface.co/nlpaueb/legal-bert-base-uncased) does not seem to be a good option as it was trained only on English data.\n",
    "          - [JuriBERT](https://huggingface.co/dascim/juribert-base) for legal texts in French. Paper explaining the model: [JuriBERT: A Masked-Language Model Adaptation for French Legal Text](https://arxiv.org/pdf/2110.01485).\n",
    "          - [ITALIAN-LEGAL-BERT](https://huggingface.co/dlicari/Italian-Legal-BERT) for legal text in Italian. Paper explaining the model: [ITALIAN-LEGAL-BERT models for improving natural language processing tasks in the Italian legal domain](https://www.sciencedirect.com/science/article/pii/S0267364923001188).\n",
    "   - Fine-tuning Setup.\n",
    "     - Freeze or unfreeze last n encoder layers—try both.\n",
    "     - Small learning rate (2e-5 – 5e-5), batch size = 8 or 16.\n",
    "     - Early stopping on validation loss.\n",
    "1. Training & Evaluation\n",
    "    - Run your k-fold CV training loops.\n",
    "    - Track accuracy, F1, precision, recall per fold.\n",
    "    - Report mean ± std.\n",
    "2. Error Analysis\n",
    "    - Look at which examples are mispredicted.\n",
    "    - Check language breakdown (FR vs. IT) to see if one is harder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaca434",
   "metadata": {},
   "source": [
    "# 2. Dataset Augmentation\n",
    "\n",
    "Outline of the intermediate tasks: We want a fully automated pipeline. A good candidate is Easy Data Augmentation (EDA) or back-translation via open‐source MT models.\n",
    "1. Choose Technique(s)\n",
    "   - EDA: random synonym substitution (WordNet or fastText), random swap, insertion, deletion.\n",
    "   - Back-translation: FR → EN → FR and IT → EN → IT using MarianMT or opus-MT.\n",
    "2. Implement & Generate\n",
    "   - For each of the 32 labeled examples, generate k augmented pseudo-examples (e.g. k=5).\n",
    "   - Deduplicate and filter (e.g. reject if new text <50% overlap).\n",
    "3. Merge & Re-split\n",
    "   - Combine original 32 + synthetic N = 32×k examples.\n",
    "   - Re-run the same CV split strategy, ensuring augmented copies of a given original stay in the same fold.\n",
    "4. Re-train BERT\n",
    "   - Exactly the same hyperparams as in (a).\n",
    "   - Track performance uplift vs. the baseline.\n",
    "5. Analysis\n",
    "   - Compare metrics: ΔAccuracy, ΔF1.\n",
    "   - Ablation: EDA vs. back-translation vs. combined.\n",
    "   - Qualitative: inspect a few synthetic samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ccf8a",
   "metadata": {},
   "source": [
    "# 3. Zero-Shot Learning with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb57e3",
   "metadata": {},
   "source": [
    "# 4. Data Generation with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ad1cc",
   "metadata": {},
   "source": [
    "# 5. Optimal Technique Application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bse-nlp-YMIcxN07-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
