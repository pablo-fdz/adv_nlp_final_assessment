{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e6bfb6",
   "metadata": {},
   "source": [
    "# 0. Instructions and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc153a5d",
   "metadata": {},
   "source": [
    "## 0.1. Instructions. Part 2: Data Scientist Challenge (3.5 points)\n",
    "\n",
    "- **Objective:** Explore different techniques to enhance model performance with limited  labeled data. You will be limited to 32 labeled examples in your task.  The rest can be viewed as unlabelled data. \n",
    "\n",
    "- **Tasks:**\n",
    "  - **a. BERT Model with Limited Data (0.5 points):** Train a BERT-based model using only 32 labeled examples and assess its performance.\n",
    "  - **b. Dataset Augmentation (1 point):** Experiment with an automated technique to increase your dataset size **without using LLMs** (chatGPT / Mistral / Gemini / etc...). Evaluate the impact on model performance.\n",
    "  - **c. Zero-Shot Learning with LLM (0.5 points):** Apply a LLM (chatGPT/Claude/Mistral/Gemini/...) in a zero-shot learning setup. Document the performance.\n",
    "  - **d. Data Generation with LLM (1 point):** Use a LLM (chatGPT/Claude/Mistral/Gemini/...) to generate new, labeled  dataset points. Train your BERT model with it + the 32 labels. Analyze  how this impacts model metrics.\n",
    "  - **e. Optimal Technique Application (0.5 points):** Based on the previous experiments, apply the most effective  technique(s) to further improve your model's performance. Comment your results and propose improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d48a5",
   "metadata": {},
   "source": [
    "## 0.2. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f98abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install polars  # Install polars for faster data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e6d715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 22:50:41.984965: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-12 22:50:41.993689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749761442.004552   70266 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749761442.007620   70266 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749761442.016628   70266 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749761442.016653   70266 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749761442.016656   70266 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749761442.016657   70266 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-12 22:50:42.019839: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Utilities\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from library.metrics import Metrics\n",
    "from library.utilities import set_seed\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning and NLP\n",
    "import torch\n",
    "import setfit\n",
    "from transformers import (AutoTokenizer, AutoModel,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer,\n",
    "                          EarlyStoppingCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "570741d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the metrics object to save the results\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f7babdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check the availability of a GPU\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280eac7",
   "metadata": {},
   "source": [
    "## 0.3. Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a2aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42. This ensures reproducibility of results across runs.\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b700f",
   "metadata": {},
   "source": [
    "## 0.4. Loading the data: Swiss Judgement Prediction\n",
    "\n",
    "Source: https://huggingface.co/datasets/rcds/swiss_judgment_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ea67e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded DataFrame shape: (35386, 9)\n",
      "\n",
      "Loaded DataFrame schema:\n",
      "Schema({'id': Int32, 'year': Int32, 'text': String, 'labels': Int64, 'language': String, 'region': String, 'canton': String, 'legal area': String, 'split': String})\n",
      "\n",
      "First few rows of the loaded DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>year</th><th>text</th><th>labels</th><th>language</th><th>region</th><th>canton</th><th>legal area</th><th>split</th></tr><tr><td>i32</td><td>i32</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>22014</td><td>2011</td><td>&quot;Faits: A. Le 28 octobre 2002 à…</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;ge&quot;</td><td>&quot;civil law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>11593</td><td>2007</td><td>&quot;Faits : Faits : A. Le 17 avril…</td><td>1</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;ge&quot;</td><td>&quot;penal law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>26670</td><td>2013</td><td>&quot;Faits: A. Par jugement du 2 ma…</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;vd&quot;</td><td>&quot;penal law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>5864</td><td>2004</td><td>&quot;Faits: Faits: A. N._, née en 1…</td><td>1</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;vd&quot;</td><td>&quot;insurance law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>16122</td><td>2009</td><td>&quot;Faits: A. Y._ est propriétaire…</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;ge&quot;</td><td>&quot;public law&quot;</td><td>&quot;train&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌───────┬──────┬──────────────────┬────────┬───┬──────────────────┬────────┬───────────────┬───────┐\n",
       "│ id    ┆ year ┆ text             ┆ labels ┆ … ┆ region           ┆ canton ┆ legal area    ┆ split │\n",
       "│ ---   ┆ ---  ┆ ---              ┆ ---    ┆   ┆ ---              ┆ ---    ┆ ---           ┆ ---   │\n",
       "│ i32   ┆ i32  ┆ str              ┆ i64    ┆   ┆ str              ┆ str    ┆ str           ┆ str   │\n",
       "╞═══════╪══════╪══════════════════╪════════╪═══╪══════════════════╪════════╪═══════════════╪═══════╡\n",
       "│ 22014 ┆ 2011 ┆ Faits: A. Le 28  ┆ 0      ┆ … ┆ Région lémanique ┆ ge     ┆ civil law     ┆ train │\n",
       "│       ┆      ┆ octobre 2002 à…  ┆        ┆   ┆                  ┆        ┆               ┆       │\n",
       "│ 11593 ┆ 2007 ┆ Faits : Faits :  ┆ 1      ┆ … ┆ Région lémanique ┆ ge     ┆ penal law     ┆ train │\n",
       "│       ┆      ┆ A. Le 17 avril…  ┆        ┆   ┆                  ┆        ┆               ┆       │\n",
       "│ 26670 ┆ 2013 ┆ Faits: A. Par    ┆ 0      ┆ … ┆ Région lémanique ┆ vd     ┆ penal law     ┆ train │\n",
       "│       ┆      ┆ jugement du 2    ┆        ┆   ┆                  ┆        ┆               ┆       │\n",
       "│       ┆      ┆ ma…              ┆        ┆   ┆                  ┆        ┆               ┆       │\n",
       "│ 5864  ┆ 2004 ┆ Faits: Faits: A. ┆ 1      ┆ … ┆ Région lémanique ┆ vd     ┆ insurance law ┆ train │\n",
       "│       ┆      ┆ N._, née en 1…   ┆        ┆   ┆                  ┆        ┆               ┆       │\n",
       "│ 16122 ┆ 2009 ┆ Faits: A. Y._    ┆ 0      ┆ … ┆ Région lémanique ┆ ge     ┆ public law    ┆ train │\n",
       "│       ┆      ┆ est              ┆        ┆   ┆                  ┆        ┆               ┆       │\n",
       "│       ┆      ┆ propriétaire…    ┆        ┆   ┆                  ┆        ┆               ┆       │\n",
       "└───────┴──────┴──────────────────┴────────┴───┴──────────────────┴────────┴───────────────┴───────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned Parquet file\n",
    "df = pl.read_parquet('swiss_judgment_prediction_fr&it_clean.parquet')\n",
    "\n",
    "# Display the loaded DataFrame\n",
    "print(\"\\nLoaded DataFrame shape:\", df.shape)\n",
    "print(\"\\nLoaded DataFrame schema:\")\n",
    "print(df.schema)\n",
    "print(\"\\nFirst few rows of the loaded DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f926bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into training, validation and test sets\n",
    "train_df = df.filter(pl.col('split') == 'train')\n",
    "val_df = df.filter(pl.col('split') == 'validation')\n",
    "test_df = df.filter(pl.col('split') == 'test')\n",
    "\n",
    "# Filter rows to keep only French language data\n",
    "train_df_fr = train_df.filter(pl.col('language') == 'fr')\n",
    "val_df_fr = val_df.filter(pl.col('language') == 'fr')\n",
    "test_df_fr = test_df.filter(pl.col('language') == 'fr')\n",
    "\n",
    "# Delete the original data to free up memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c4b4b",
   "metadata": {},
   "source": [
    "# 1. BERT Model with Limited Data\n",
    "\n",
    "Outline of the intermediate tasks:\n",
    "\n",
    "1. Preprocessing Pipeline\n",
    "   - Lowercasing, punctuation stripping (or not, depending on BERT tokenizer).\n",
    "   - Sentencepiece/BPE tokenization via the CamemBERT (for French) or UmBERTo (for Italian).\n",
    "   - (Optional) language tags if you merge FR+IT in one model.\n",
    "2. Hold-out Split. Since you only have 32 labels: use stratified k-fold CV (e.g. 8 × 4-fold) to get reliable estimates, or leave-one-out if you want maximum training data per fold.\n",
    "3. BERT Model with Only 32 Examples\n",
    "   - Model Choice: Pick a multilingual BERT (mBERT) or separate CamemBERT/UmBERTo checkpoint. Alternatives:\n",
    "     - Multilingual/monolingual models:\n",
    "       - One multilingual [`Sentence-Transformer` model](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#original-models), such as `paraphrase-multilingual-mpnet-base-v2` (best multilingual performer), `paraphrase-multilingual-MiniLM-L12-v2` (similar performer as the former, but much faster and smaller) or `distiluse-base-multilingual-cased-v1` (worst of the bunch).\n",
    "       -  [BERT multilingual base model (cased)](https://huggingface.co/google-bert/bert-base-multilingual-cased). [Uncased model](https://huggingface.co/google-bert/bert-base-multilingual-uncased) also available. For an even smaller model, the distilled version can be used: [`distilbert-base-multilingual-cased`](https://huggingface.co/distilbert/distilbert-base-multilingual-cased).\n",
    "       -  [CamemBERT 2.0](https://huggingface.co/almanach/camembertv2-base) and [CamemBERTav2](https://huggingface.co/almanach/camembertav2-base), models trained with French text and explained in the paper: [CamemBERT 2.0: A Smarter French Language Model Aged to Perfection](https://arxiv.org/html/2411.08868v1#S3). These models supposedly improve over the performance of the original [CamemBERT](https://huggingface.co/almanach/camembert-base) model, explained in \"[CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)\".\n",
    "       -  [FlauBERT](https://huggingface.co/docs/transformers/en/model_doc/flaubert), another model pre-trained on French text. Paper: \"[FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372)\". \n",
    "       -  [BERT Base Italian Cased](https://huggingface.co/dbmdz/bert-base-italian-cased), [Uncased](https://huggingface.co/dbmdz/bert-base-italian-uncased), ,and [XXL Uncased](https://huggingface.co/dbmdz/bert-base-italian-xxl-uncased). \n",
    "       -  [UmBERTo Commoncrawl Cased](https://huggingface.co/Musixmatch/umberto-commoncrawl-cased-v1), another model trained with a large corpus of texts in Italian.\n",
    "      -  Domain-specific models (law):\n",
    "          - [LEGAL-BERT](https://huggingface.co/nlpaueb/legal-bert-base-uncased) does not seem to be a good option as it was trained only on English data.\n",
    "          - [JuriBERT](https://huggingface.co/dascim/juribert-base) for legal texts in French. Paper explaining the model: [JuriBERT: A Masked-Language Model Adaptation for French Legal Text](https://arxiv.org/pdf/2110.01485).\n",
    "          - [ITALIAN-LEGAL-BERT](https://huggingface.co/dlicari/Italian-Legal-BERT) for legal text in Italian. Paper explaining the model: [ITALIAN-LEGAL-BERT models for improving natural language processing tasks in the Italian legal domain](https://www.sciencedirect.com/science/article/pii/S0267364923001188).\n",
    "     - Models to try: \n",
    "       - For both languages: `paraphrase-multilingual-MiniLM-L12-v2` (of the SentenceTransformers, good balance between performance and speed), BERT multilingual base model (cased).\n",
    "       - For French: CamemBERT, FlauBERT, CamemBERTav2 (based on DebertaV2 architecture), JuriBERT.\n",
    "       - For Italian: BERT Base Italian Cased, UmBERTo Commoncrawl Cased, ITALIAN-LEGAL-BERT.\n",
    "     - [Model memory estimator](https://huggingface.co/docs/accelerate/en/usage_guides/model_size_estimator): in order to know whether a model actually fits in the memory of your computer.\n",
    "   - Fine-tuning Setup.\n",
    "     - Freeze or unfreeze last n encoder layers—try both.\n",
    "     - Small learning rate (2e-5 – 5e-5), batch size = 8 or 16.\n",
    "     - Early stopping on validation loss.\n",
    "4. Training & Evaluation\n",
    "    - Run your k-fold CV training loops.\n",
    "    - Track accuracy, F1, precision, recall per fold.\n",
    "    - Report mean ± std.\n",
    "5. Error Analysis and feature interpretation.\n",
    "    - Use `LIME` for analyzing the most relevant features for classifying the texts. \n",
    "    - Look at which examples are mispredicted.\n",
    "    - Check language breakdown (FR vs. IT) to see if one is harder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e98272",
   "metadata": {},
   "source": [
    "## 1.1. Using SetFit (\"Sentence Transformer Fine-Tuning\")\n",
    "\n",
    "Notebook of reference: `Session_6_2_Zero_Shot_Classification.ipynb`, introduction, step 1 (loading data) and step 6 (\"Few-Shot Classification with SetFit\").\n",
    "\n",
    "Applying SetFit (the “Sentence Transformer Fine-Tuning” recipe) can be regarded as **training** (it fine-tunes a pre-trained sentence-embedding model, plus fits a small classifier on top). Furthermore, SetFit was built **for** getting strong performance with as few as a few dozen labeled examples.\n",
    "\n",
    "---\n",
    "\n",
    "**Why SetFit can be regarded as training**\n",
    "\n",
    "- **Contrastive fine-tuning:**\n",
    "  We start with a frozen (or lightly unfrozen) Sentence-Transformer model and then *fine-tune* it on automatically generated sentence pairs derived from your 32 labels.\n",
    "- **Classifier head training:**\n",
    "  After contrastive tuning, SetFit fits a lightweight logistic-regression (or small MLP) classifier on the resulting embeddings.\n",
    "- Both steps update model parameters—so it’s training/fine-tuning, not mere prompt-engineering or zero-shot.\n",
    "\n",
    "---\n",
    "\n",
    "**Why SetFit excels in limited-label regimes**\n",
    "\n",
    "1. **Data amplification via contrastive pairs**\n",
    "\n",
    "   - From each labeled example, SetFit creates positive pairs (e.g. two different augmentations of the same sentence) and negative pairs (across classes), turning 32 labels into hundreds or thousands of pairwise signals.\n",
    "   - That extra signal helps the embedding space separate classes, even when you only have a few “gold” labels.\n",
    "\n",
    "1. **Lightweight classifier**\n",
    "\n",
    "   * Because the embedding model has already been tuned to distinguish the classes, the final classifier can be a simple logistic or MLP—so it needs very few examples to learn decision boundaries.\n",
    "\n",
    "2. **Empirical few-shot strength**\n",
    "\n",
    "   * In benchmarks, SetFit often outperforms standard BERT fine-tuning with few labels, and it’s much faster to train (no full back-prop through all BERT layers).\n",
    "\n",
    "---\n",
    "\n",
    "Note that there are 2 aspects that we need to implement for SetFit to work properly:\n",
    "\n",
    "1. **Balance the examples of positive and negative classes** that will be passed for contrastive learning. Since approximately 29% of the labels are positive (approved motions) and the other 71% correspond to rejected motions, passing examples for contrastive learning without balancing the clases would bias the embedding space towards the majority class.\n",
    "   \n",
    "2. We need to **run several iterations** in order to get lower variability in the results, as a single training run can be highly variable due to randomness in the example sampling (as we only pass 32 labelled examples). Therefore, if we don't run several iterations for SetFit, the results would be highly noisy. Averaging across iterations smooths out this variance and gives us the true expected performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689690d",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57146be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample balanced dataset\n",
    "def sample_balanced_dataset(dataset: pl.DataFrame, num_samples, seed):\n",
    "\n",
    "    \"\"\"\n",
    "    Sample a balanced dataset with equal numbers of positive and negative examples.\n",
    "    \n",
    "    Args:\n",
    "        dataset (pl.DataFrame): The input dataset containing 'text' and 'label' columns.\n",
    "        num_samples (int): Total number of samples to return, must be even.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        Dataset: A balanced Dataset object with equal numbers of positive and negative examples.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get positive and negative examples\n",
    "    pos_examples = dataset.filter(pl.col('label') == 1)\n",
    "    neg_examples = dataset.filter(pl.col('label') == 0)\n",
    "    \n",
    "    # Sample equal numbers from each class\n",
    "    samples_per_class = num_samples // 2\n",
    "    \n",
    "    if seed is not None:\n",
    "        pos_sampled = pos_examples.sample(n=samples_per_class, shuffle=True, seed=seed)\n",
    "        neg_sampled = neg_examples.sample(n=samples_per_class, shuffle=True, seed=seed)\n",
    "    else:\n",
    "        raise ValueError(\"Seed must be provided for reproducibility.\")\n",
    "    \n",
    "    # Concatenate the sampled DataFrames, only the text and the label columns\n",
    "    pos_sampled = pos_sampled.select(['text', 'label'])\n",
    "    neg_sampled = neg_sampled.select(['text', 'label'])\n",
    "    df = pl.concat([pos_sampled, neg_sampled], how='vertical')\n",
    "\n",
    "    # Combine the datasets into a single Dataset object\n",
    "    combined = Dataset.from_polars(df)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94d42a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run SetFit training and evaluation routine\n",
    "def run_setfit_training(train_df: pl.DataFrame, val_df: pl.DataFrame, \n",
    "                        model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "                        num_epochs=5, batch_size=16, learning_rate=2e-5, sample_size=32,\n",
    "                        metric='f1', num_iterations=10, seed=42):\n",
    "    \"\"\"\n",
    "    Run SetFit training and evaluation routine.\n",
    "    \n",
    "    Args:\n",
    "        train_df (pl.DataFrame): Training DataFrame with 'text' and 'label' columns.\n",
    "        val_df (pl.DataFrame): Validation DataFrame with 'text' and 'label' columns.\n",
    "        model_name (str): Pretrained model name for SetFit.\n",
    "        num_epochs (int): Number of epochs for training.\n",
    "        batch_size (int): Batch size for training.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        sample_size (int): Number of samples to use for training in each iteration.\n",
    "        metric (str): Metric to optimize during training ('f1', 'accuracy', etc.).\n",
    "        num_iterations (int): Number of iterations to run the training process.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing evaluation metrics for each iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the validation set\n",
    "    val_set = Dataset.from_polars(val_df.select(['text', 'label']))\n",
    "\n",
    "    # Store results across iterations (for different metrics and iterations)\n",
    "    iteration_results = []\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    best_iteration = 0\n",
    "\n",
    "    # Run the sampling and training process with SetFit\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nIteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "        # Create a fresh model for each iteration to avoid contamination\n",
    "        model = setfit.SetFitModel.from_pretrained(model_name)\n",
    "\n",
    "        # Sample balanced training data\n",
    "        train_samples = sample_balanced_dataset(train_df, sample_size, seed + iteration)\n",
    "\n",
    "        # Create the training arguments\n",
    "        train_args = setfit.TrainingArguments(\n",
    "            num_epochs=(num_epochs, num_epochs),  # Tuple format: (sentence_transformer_epochs, head_epochs)\n",
    "            batch_size=(batch_size, batch_size),  # Tuple format: (sentence_transformer_batch, head_batch)\n",
    "            body_learning_rate=(learning_rate, learning_rate),  # Tuple format for body learning rate (first for sentence transformer, second for head classifier)\n",
    "        )\n",
    "\n",
    "        # Initialize and train SetFit model\n",
    "        trainer = setfit.Trainer(\n",
    "            model=model,\n",
    "            train_dataset=train_samples,  # Pairs of text and labels for Contrastive Learning\n",
    "            eval_dataset=val_set,  # Validation set for evaluation\n",
    "            metric=metric,  # Metric to optimize\n",
    "            args=train_args  # Training arguments\n",
    "        )\n",
    "\n",
    "        trainer.train()  # Train the model\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "        # Evaluate on validation set (for hyperparameter tuning/model selection)\n",
    "        val_predictions = trainer.model.predict(val_set['text'])\n",
    "        val_metrics = {\n",
    "            'accuracy': accuracy_score(val_set['label'], val_predictions),\n",
    "            'f1': f1_score(val_set['label'], val_predictions),\n",
    "            'precision': precision_score(val_set['label'], val_predictions),\n",
    "            'recall': recall_score(val_set['label'], val_predictions)\n",
    "        }\n",
    "        \n",
    "        # Store results for this iteration\n",
    "        iteration_results.append(val_metrics)\n",
    "        print(f\"Validation F1: {val_metrics['f1']:.4f}\")\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        current_score = val_metrics[metric]\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_iteration = iteration + 1\n",
    "            # Clean up previous best model\n",
    "            if best_model is not None:\n",
    "                del best_model\n",
    "            # Store reference to current best model\n",
    "            best_model = trainer.model\n",
    "            print(f\"New best model found with {metric}: {best_score:.4f}\")\n",
    "\n",
    "        # Clean up memory for this iteration - SINGLE, CLEAR LOGIC\n",
    "        if best_model is trainer.model:\n",
    "            # This is the best model, only delete trainer\n",
    "            del model, trainer\n",
    "        else:\n",
    "            # This is not the best model, delete everything\n",
    "            del model, trainer\n",
    "\n",
    "        # Clear CUDA cache if using GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()  # Wait for all operations to complete\n",
    "    \n",
    "    print('Finished training in all iterations. Saving the results to a parquet file...')\n",
    "    \n",
    "    # Save the best model\n",
    "    san_model_name = model_name.split(sep='/')[-1]  # Sanitize model name for file path, keep only the last part\n",
    "    model_path = os.path.join('models', 'part_2', 'a', f'setfit_best_{san_model_name}')\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    best_model.save_pretrained(model_path)  # Save the best model to the specified path\n",
    "    print(f'Best model saved to: {model_path}')\n",
    "\n",
    "    # Save the results in a polars DataFrame\n",
    "    results_df = pl.DataFrame(iteration_results)\n",
    "    results_df = results_df.with_columns(pl.Series(\"iteration\", range(1, len(results_df) + 1)))  # Add iteration number to the results DataFrame\n",
    "\n",
    "    # Save the results DataFrame to a Parquet file\n",
    "    results_path = os.path.join('results', 'part_2', 'a', f'setfit_results_{san_model_name}.parquet')\n",
    "    os.makedirs(os.path.dirname(results_path), exist_ok=True)  # Ensure the directory exists\n",
    "    results_df.write_parquet(results_path)\n",
    "    print(f'Results saved to: {results_path}')\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fbdb10",
   "metadata": {},
   "source": [
    "Note that SetFit only natively supports sentence-transformer models: passing one which is [not](https://huggingface.co/models?library=sentence-transformers&author=sentence-transformers) (see also the [docs](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#original-models)) will make SetFit automatically wrap the BERT model with a sentence-transformers layer using mean pooling. Using a model that is not part of the `sentence-transformers` will yield the following message:\n",
    "\n",
    "> No sentence-transformers model found with name google-bert/bert-base-multilingual-cased. Creating a new one with mean pooling.\n",
    "\n",
    "However, for better performance with SetFit, it is preferred to use a model that was specifically pre-trained as a sentence transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3461c7",
   "metadata": {},
   "source": [
    "   - Model Choice: Pick a multilingual BERT (mBERT) or separate CamemBERT/UmBERTo checkpoint. Alternatives:\n",
    "     - Multilingual/monolingual models:\n",
    "       - One multilingual [`Sentence-Transformer` model](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#original-models), such as `paraphrase-multilingual-mpnet-base-v2` (best multilingual performer), `paraphrase-multilingual-MiniLM-L12-v2` (similar performer as the former, but much faster and smaller) or `distiluse-base-multilingual-cased-v1` (worst of the bunch).\n",
    "       -  [BERT multilingual base model (cased)](https://huggingface.co/google-bert/bert-base-multilingual-cased). [Uncased model](https://huggingface.co/google-bert/bert-base-multilingual-uncased) also available. Paper: \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\". \n",
    "       -  [CamemBERT 2.0](https://huggingface.co/almanach/camembertv2-base) and [CamemBERTav2](https://huggingface.co/almanach/camembertav2-base), models trained with French text and explained in the paper: [CamemBERT 2.0: A Smarter French Language Model Aged to Perfection](https://arxiv.org/html/2411.08868v1#S3). These models supposedly improve over the performance of the original [CamemBERT](https://huggingface.co/docs/transformers/en/model_doc/camembert) model, explained in \"[CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)\".\n",
    "       -  [FlauBERT](https://huggingface.co/docs/transformers/en/model_doc/flaubert), another model pre-trained on French text. Paper: \"[FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372)\". \n",
    "       -  [BERT Base Italian Cased](https://huggingface.co/dbmdz/bert-base-italian-cased), [Uncased](https://huggingface.co/dbmdz/bert-base-italian-uncased), ,and [XXL Uncased](https://huggingface.co/dbmdz/bert-base-italian-xxl-uncased). \n",
    "       -  [UmBERTo Commoncrawl Cased](https://huggingface.co/Musixmatch/umberto-commoncrawl-cased-v1), another model trained with a large corpus of texts in Italian.\n",
    "      -  Domain-specific models (law):\n",
    "          - [LEGAL-BERT](https://huggingface.co/nlpaueb/legal-bert-base-uncased) does not seem to be a good option as it was trained only on English data.\n",
    "          - [JuriBERT](https://huggingface.co/dascim/juribert-base) for legal texts in French. Paper explaining the model: [JuriBERT: A Masked-Language Model Adaptation for French Legal Text](https://arxiv.org/pdf/2110.01485).\n",
    "          - [ITALIAN-LEGAL-BERT](https://huggingface.co/dlicari/Italian-Legal-BERT) for legal text in Italian. Paper explaining the model: [ITALIAN-LEGAL-BERT models for improving natural language processing tasks in the Italian legal domain](https://www.sciencedirect.com/science/article/pii/S0267364923001188).\n",
    "\n",
    "Models to try: \n",
    "- For both languages: `paraphrase-multilingual-MiniLM-L12-v2` (of the SentenceTransformers, good balance between performance and speed), distilled BERT multilingual base model (cased) ([`distilbert-base-multilingual-cased`](https://huggingface.co/distilbert/distilbert-base-multilingual-cased).)\n",
    "- For French: CamemBERTav2 (based on DebertaV2 architecture), JuriBERT.\n",
    "- For Italian: BERT Base Italian Cased, ITALIAN-LEGAL-BERT.\n",
    "\n",
    "[Model memory estimator](https://huggingface.co/docs/accelerate/en/usage_guides/model_size_estimator): in order to know whether a model actually fits in the memory of your computer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de3244",
   "metadata": {},
   "source": [
    "Finally, consider that there might be some memory issues by running SetFit:  ( https://github.com/huggingface/setfit/issues/472 ). Adapt the hyperparameters consequently to your memory limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50947e25",
   "metadata": {},
   "source": [
    "### Multilingual models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72ad9b",
   "metadata": {},
   "source": [
    "#### `paraphrase-multilingual-MiniLM-L12-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match SetFit requirements (labels should be 'labels'), creating a copy\n",
    "train_df_setfit = train_df.clone()\n",
    "val_df_setfit = val_df.clone()\n",
    "train_df_setfit = train_df_setfit.rename({'labels': 'label'})\n",
    "val_df_setfit = val_df_setfit.rename({'labels': 'label'})\n",
    "\n",
    "# Define sample size (labelled examples for training)\n",
    "sample_size = 32\n",
    "num_iterations = 10  # Run several iterations for the sample size (32 labels) to minimize the impact of randomness\n",
    "metric = 'f1'  # Metric to optimize\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"  # Path to the pre-trained model\n",
    "batch_size = 64 # Batch size for training (reduce if you run into memory issues, but larger batch sizes will speed up training)\n",
    "num_epochs = 5  # Number of epochs for training\n",
    "learning_rate= 2e-5  # Learning rate for the optimizer\n",
    "\n",
    "# Create a subset of the validation data frame for faster processing\n",
    "sample_size_val = 500\n",
    "val_df_setfit = val_df_setfit.sample(n=sample_size_val, shuffle=True, seed=seed)\n",
    "\n",
    "results_m1 = run_setfit_training(\n",
    "    train_df=train_df_setfit, \n",
    "    val_df=val_df_setfit, \n",
    "    model_name=model_name, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    learning_rate=learning_rate, \n",
    "    sample_size=sample_size, \n",
    "    metric=metric, \n",
    "    num_iterations=num_iterations, \n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34d2bce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>accuracy</th><th>f1</th><th>precision</th><th>recall</th><th>iteration</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>0.55</td><td>0.285714</td><td>0.204545</td><td>0.473684</td><td>1</td></tr><tr><td>0.546</td><td>0.250825</td><td>0.182692</td><td>0.4</td><td>2</td></tr><tr><td>0.49</td><td>0.25656</td><td>0.177419</td><td>0.463158</td><td>3</td></tr><tr><td>0.52</td><td>0.259259</td><td>0.183406</td><td>0.442105</td><td>4</td></tr><tr><td>0.458</td><td>0.306905</td><td>0.202703</td><td>0.631579</td><td>5</td></tr><tr><td>0.502</td><td>0.233846</td><td>0.165217</td><td>0.4</td><td>6</td></tr><tr><td>0.544</td><td>0.283019</td><td>0.201794</td><td>0.473684</td><td>7</td></tr><tr><td>0.436</td><td>0.26178</td><td>0.174216</td><td>0.526316</td><td>8</td></tr><tr><td>0.384</td><td>0.283721</td><td>0.18209</td><td>0.642105</td><td>9</td></tr><tr><td>0.58</td><td>0.227941</td><td>0.175141</td><td>0.326316</td><td>10</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 5)\n",
       "┌──────────┬──────────┬───────────┬──────────┬───────────┐\n",
       "│ accuracy ┆ f1       ┆ precision ┆ recall   ┆ iteration │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---      ┆ ---       │\n",
       "│ f64      ┆ f64      ┆ f64       ┆ f64      ┆ i64       │\n",
       "╞══════════╪══════════╪═══════════╪══════════╪═══════════╡\n",
       "│ 0.55     ┆ 0.285714 ┆ 0.204545  ┆ 0.473684 ┆ 1         │\n",
       "│ 0.546    ┆ 0.250825 ┆ 0.182692  ┆ 0.4      ┆ 2         │\n",
       "│ 0.49     ┆ 0.25656  ┆ 0.177419  ┆ 0.463158 ┆ 3         │\n",
       "│ 0.52     ┆ 0.259259 ┆ 0.183406  ┆ 0.442105 ┆ 4         │\n",
       "│ 0.458    ┆ 0.306905 ┆ 0.202703  ┆ 0.631579 ┆ 5         │\n",
       "│ 0.502    ┆ 0.233846 ┆ 0.165217  ┆ 0.4      ┆ 6         │\n",
       "│ 0.544    ┆ 0.283019 ┆ 0.201794  ┆ 0.473684 ┆ 7         │\n",
       "│ 0.436    ┆ 0.26178  ┆ 0.174216  ┆ 0.526316 ┆ 8         │\n",
       "│ 0.384    ┆ 0.283721 ┆ 0.18209   ┆ 0.642105 ┆ 9         │\n",
       "│ 0.58     ┆ 0.227941 ┆ 0.175141  ┆ 0.326316 ┆ 10        │\n",
       "└──────────┴──────────┴───────────┴──────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean metrics across all iterations:\n",
      "{'accuracy': [0.5010000000000001], 'f1': [0.26495709982755045], 'precision': [0.1849223869644958], 'recall': [0.47789473684210526]}\n"
     ]
    }
   ],
   "source": [
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "results_m1 = pl.read_parquet(os.path.join('results', 'part_2', 'a', f'setfit_results_{model_name}.parquet'))\n",
    "\n",
    "print(f'Results for model: {model_name}')\n",
    "display(results_m1)\n",
    "\n",
    "# Print mean of all metrics across iterations\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "mean_metrics = results_m1.select([pl.col(metric).mean().alias(metric) for metric in metrics]).to_dict(as_series=False)\n",
    "print(\"\\nMean metrics across all iterations:\")\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b7c5a",
   "metadata": {},
   "source": [
    "#### BERT-base multilingual cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88078b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match SetFit requirements (labels should be 'labels'), creating a copy\n",
    "train_df_setfit = train_df.clone()\n",
    "val_df_setfit = val_df.clone()\n",
    "train_df_setfit = train_df_setfit.rename({'labels': 'label'})\n",
    "val_df_setfit = val_df_setfit.rename({'labels': 'label'})\n",
    "\n",
    "# Define sample size (labelled examples for training)\n",
    "sample_size = 32\n",
    "num_iterations = 5  # Run several iterations for the sample size (32 labels) to minimize the impact of randomness\n",
    "metric = 'f1'  # Metric to optimize\n",
    "model_name = \"google-bert/bert-base-multilingual-cased\"  # Path to the pre-trained model\n",
    "batch_size = 8 # Batch size for training (reduce if you run into memory issues, but larger batch sizes will speed up training)\n",
    "num_epochs = 5  # Number of epochs for training\n",
    "learning_rate= 2e-5  # Learning rate for the optimizer\n",
    "\n",
    "# Create a subset of the validation data frame for faster processing\n",
    "sample_size_val = 500\n",
    "val_df_setfit = val_df_setfit.sample(n=sample_size_val, shuffle=True, seed=seed)\n",
    "\n",
    "results_m2 = run_setfit_training(\n",
    "    train_df=train_df_setfit, \n",
    "    val_df=val_df_setfit, \n",
    "    model_name=model_name, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    learning_rate=learning_rate, \n",
    "    sample_size=sample_size, \n",
    "    metric=metric, \n",
    "    num_iterations=num_iterations, \n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae1d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model: google-bert/bert-base-multilingual-cased\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>accuracy</th><th>f1</th><th>precision</th><th>recall</th><th>iteration</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>0.532</td><td>0.26875</td><td>0.191111</td><td>0.452632</td><td>1</td></tr><tr><td>0.42</td><td>0.299517</td><td>0.194357</td><td>0.652632</td><td>2</td></tr><tr><td>0.61</td><td>0.229249</td><td>0.183544</td><td>0.305263</td><td>3</td></tr><tr><td>0.464</td><td>0.298429</td><td>0.198606</td><td>0.6</td><td>4</td></tr><tr><td>0.496</td><td>0.296089</td><td>0.201521</td><td>0.557895</td><td>5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────┬──────────┬───────────┬──────────┬───────────┐\n",
       "│ accuracy ┆ f1       ┆ precision ┆ recall   ┆ iteration │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---      ┆ ---       │\n",
       "│ f64      ┆ f64      ┆ f64       ┆ f64      ┆ i64       │\n",
       "╞══════════╪══════════╪═══════════╪══════════╪═══════════╡\n",
       "│ 0.532    ┆ 0.26875  ┆ 0.191111  ┆ 0.452632 ┆ 1         │\n",
       "│ 0.42     ┆ 0.299517 ┆ 0.194357  ┆ 0.652632 ┆ 2         │\n",
       "│ 0.61     ┆ 0.229249 ┆ 0.183544  ┆ 0.305263 ┆ 3         │\n",
       "│ 0.464    ┆ 0.298429 ┆ 0.198606  ┆ 0.6      ┆ 4         │\n",
       "│ 0.496    ┆ 0.296089 ┆ 0.201521  ┆ 0.557895 ┆ 5         │\n",
       "└──────────┴──────────┴───────────┴──────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'bert-base-multilingual-cased'\n",
    "\n",
    "results_m2 = pl.read_parquet(os.path.join('results', 'part_2', 'a', f'setfit_results_{model_name}.parquet'))\n",
    "\n",
    "print(f'Results for model: {model_name}')\n",
    "display(results_m2)\n",
    "\n",
    "# Print mean of all metrics across iterations\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "mean_metrics = results_m2.select([pl.col(metric).mean().alias(metric) for metric in metrics]).to_dict(as_series=False)\n",
    "print(\"\\nMean metrics across all iterations:\")\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c77b75",
   "metadata": {},
   "source": [
    "### Models pre-trained in French"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fede7",
   "metadata": {},
   "source": [
    "#### CamemBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347dbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match SetFit requirements (labels should be 'labels'), creating a copy\n",
    "train_fr_df = train_df.clone()\n",
    "val_fr_df = val_df.clone()\n",
    "# Keep only the columns where the language is French\n",
    "train_fr_df = train_fr_df.filter(pl.col('language') == 'fr').rename({'labels': 'label'})\n",
    "val_fr_df = val_fr_df.filter(pl.col('language') == 'fr').rename({'labels': 'label'})\n",
    "\n",
    "# Define sample size (labelled examples for training)\n",
    "sample_size = 32\n",
    "num_iterations = 5  # Run several iterations for the sample size (32 labels) to minimize the impact of randomness\n",
    "metric = 'f1'  # Metric to optimize\n",
    "model_name = \"almanach/camembert-base\"  # Path to the pre-trained model\n",
    "batch_size = 8  # Batch size for training (reduce if you run into memory issues, but larger batch sizes will speed up training)\n",
    "num_epochs = 5  # Number of epochs for training\n",
    "learning_rate= 2e-5  # Learning rate for the optimizer\n",
    "\n",
    "# Create a subset of the validation data frame for faster processing\n",
    "sample_size_val = 500\n",
    "val_fr_df = val_fr_df.sample(n=sample_size_val, shuffle=True, seed=seed)\n",
    "\n",
    "results_m3 = run_setfit_training(\n",
    "    train_df=train_fr_df, \n",
    "    val_df=val_fr_df, \n",
    "    model_name=model_name, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    learning_rate=learning_rate, \n",
    "    sample_size=sample_size, \n",
    "    metric=metric, \n",
    "    num_iterations=num_iterations, \n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ac1e825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model: camembert-base\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>accuracy</th><th>f1</th><th>precision</th><th>recall</th><th>iteration</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>0.642</td><td>0.286853</td><td>0.25</td><td>0.336449</td><td>1</td></tr><tr><td>0.644</td><td>0.245763</td><td>0.224806</td><td>0.271028</td><td>2</td></tr><tr><td>0.446</td><td>0.31941</td><td>0.216667</td><td>0.607477</td><td>3</td></tr><tr><td>0.46</td><td>0.341463</td><td>0.231023</td><td>0.654206</td><td>4</td></tr><tr><td>0.628</td><td>0.243902</td><td>0.215827</td><td>0.280374</td><td>5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────┬──────────┬───────────┬──────────┬───────────┐\n",
       "│ accuracy ┆ f1       ┆ precision ┆ recall   ┆ iteration │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---      ┆ ---       │\n",
       "│ f64      ┆ f64      ┆ f64       ┆ f64      ┆ i64       │\n",
       "╞══════════╪══════════╪═══════════╪══════════╪═══════════╡\n",
       "│ 0.642    ┆ 0.286853 ┆ 0.25      ┆ 0.336449 ┆ 1         │\n",
       "│ 0.644    ┆ 0.245763 ┆ 0.224806  ┆ 0.271028 ┆ 2         │\n",
       "│ 0.446    ┆ 0.31941  ┆ 0.216667  ┆ 0.607477 ┆ 3         │\n",
       "│ 0.46     ┆ 0.341463 ┆ 0.231023  ┆ 0.654206 ┆ 4         │\n",
       "│ 0.628    ┆ 0.243902 ┆ 0.215827  ┆ 0.280374 ┆ 5         │\n",
       "└──────────┴──────────┴───────────┴──────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best iteration metrics (highest F1-score):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>accuracy</th><th>f1</th><th>precision</th><th>recall</th><th>iteration</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>0.46</td><td>0.341463</td><td>0.231023</td><td>0.654206</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 5)\n",
       "┌──────────┬──────────┬───────────┬──────────┬───────────┐\n",
       "│ accuracy ┆ f1       ┆ precision ┆ recall   ┆ iteration │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---      ┆ ---       │\n",
       "│ f64      ┆ f64      ┆ f64       ┆ f64      ┆ i64       │\n",
       "╞══════════╪══════════╪═══════════╪══════════╪═══════════╡\n",
       "│ 0.46     ┆ 0.341463 ┆ 0.231023  ┆ 0.654206 ┆ 4         │\n",
       "└──────────┴──────────┴───────────┴──────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'camembert-base'\n",
    "\n",
    "results_m3 = pl.read_parquet(os.path.join('results', 'part_2', 'a', f'setfit_results_{model_name}.parquet'))\n",
    "\n",
    "print(f'Results for model: {model_name}')\n",
    "display(results_m3)\n",
    "\n",
    "# Print the metrics for the epoch with the lowest validation loss\n",
    "best_epoch = results_m3.filter(pl.col('f1') == results_m3['f1'].max())\n",
    "print(\"\\nBest iteration metrics (highest F1-score):\")\n",
    "display(best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458abb73",
   "metadata": {},
   "source": [
    "#### JuriBERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match SetFit requirements (labels should be 'labels'), creating a copy\n",
    "train_fr_df = train_df.clone()\n",
    "val_fr_df = val_df.clone()\n",
    "# Keep only the columns where the language is French\n",
    "train_fr_df = train_fr_df.filter(pl.col('language') == 'fr').rename({'labels': 'label'})\n",
    "val_fr_df = val_fr_df.filter(pl.col('language') == 'fr').rename({'labels': 'label'})\n",
    "\n",
    "# Define sample size (labelled examples for training)\n",
    "sample_size = 32\n",
    "num_iterations = 5  # Run several iterations for the sample size (32 labels) to minimize the impact of randomness\n",
    "metric = 'f1'  # Metric to optimize\n",
    "model_name = \"dascim/juribert-base\"  # Path to the pre-trained model\n",
    "batch_size = 8  # Batch size for training (reduce if you run into memory issues, but larger batch sizes will speed up training)\n",
    "num_epochs = 5  # Number of epochs for training\n",
    "learning_rate= 2e-5  # Learning rate for the optimizer\n",
    "\n",
    "# Create a subset of the validation data frame for faster processing\n",
    "sample_size_val = 500\n",
    "val_fr_df = val_fr_df.sample(n=sample_size_val, shuffle=True, seed=seed)\n",
    "\n",
    "results_m4 = run_setfit_training(\n",
    "    train_df=train_fr_df, \n",
    "    val_df=val_fr_df, \n",
    "    model_name=model_name, \n",
    "    num_epochs=num_epochs, \n",
    "    batch_size=batch_size, \n",
    "    learning_rate=learning_rate, \n",
    "    sample_size=sample_size, \n",
    "    metric=metric, \n",
    "    num_iterations=num_iterations, \n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b80306f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model: juribert-base\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>accuracy</th><th>f1</th><th>precision</th><th>recall</th><th>iteration</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>0.606</td><td>0.318339</td><td>0.252747</td><td>0.429907</td><td>1</td></tr><tr><td>0.418</td><td>0.285012</td><td>0.193333</td><td>0.542056</td><td>2</td></tr><tr><td>0.522</td><td>0.307246</td><td>0.222689</td><td>0.495327</td><td>3</td></tr><tr><td>0.578</td><td>0.259649</td><td>0.207865</td><td>0.345794</td><td>4</td></tr><tr><td>0.574</td><td>0.292359</td><td>0.226804</td><td>0.411215</td><td>5</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────┬──────────┬───────────┬──────────┬───────────┐\n",
       "│ accuracy ┆ f1       ┆ precision ┆ recall   ┆ iteration │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---      ┆ ---       │\n",
       "│ f64      ┆ f64      ┆ f64       ┆ f64      ┆ i64       │\n",
       "╞══════════╪══════════╪═══════════╪══════════╪═══════════╡\n",
       "│ 0.606    ┆ 0.318339 ┆ 0.252747  ┆ 0.429907 ┆ 1         │\n",
       "│ 0.418    ┆ 0.285012 ┆ 0.193333  ┆ 0.542056 ┆ 2         │\n",
       "│ 0.522    ┆ 0.307246 ┆ 0.222689  ┆ 0.495327 ┆ 3         │\n",
       "│ 0.578    ┆ 0.259649 ┆ 0.207865  ┆ 0.345794 ┆ 4         │\n",
       "│ 0.574    ┆ 0.292359 ┆ 0.226804  ┆ 0.411215 ┆ 5         │\n",
       "└──────────┴──────────┴───────────┴──────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best iteration metrics (highest F1-score):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>accuracy</th><th>f1</th><th>precision</th><th>recall</th><th>iteration</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>0.606</td><td>0.318339</td><td>0.252747</td><td>0.429907</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 5)\n",
       "┌──────────┬──────────┬───────────┬──────────┬───────────┐\n",
       "│ accuracy ┆ f1       ┆ precision ┆ recall   ┆ iteration │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---      ┆ ---       │\n",
       "│ f64      ┆ f64      ┆ f64       ┆ f64      ┆ i64       │\n",
       "╞══════════╪══════════╪═══════════╪══════════╪═══════════╡\n",
       "│ 0.606    ┆ 0.318339 ┆ 0.252747  ┆ 0.429907 ┆ 1         │\n",
       "└──────────┴──────────┴───────────┴──────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'juribert-base'\n",
    "\n",
    "results_m4 = pl.read_parquet(os.path.join('results', 'part_2', 'a', f'setfit_results_{model_name}.parquet'))\n",
    "\n",
    "print(f'Results for model: {model_name}')\n",
    "display(results_m4)\n",
    "\n",
    "# Print the metrics for the epoch with the lowest validation loss\n",
    "best_epoch = results_m4.filter(pl.col('f1') == results_m4['f1'].max())\n",
    "print(\"\\nBest iteration metrics (highest F1-score):\")\n",
    "display(best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1080bae",
   "metadata": {},
   "source": [
    "## 1.2. Standard fine-tuning\n",
    "\n",
    "Notebook of reference: `Session_5_1_BERT_HF_Implementation.ipynb`, sections 1, 2 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684a58d",
   "metadata": {},
   "source": [
    "### With CamemBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41850348",
   "metadata": {},
   "source": [
    "First, we set basic parameters that we will need for fine-tuning a model based on the BERT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319024ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: almanach/camembert-base, Max Length: 512\n"
     ]
    }
   ],
   "source": [
    "model_name = \"almanach/camembert-base\"  # Path to the pre-trained model\n",
    "num_labels = 2  # Number of labels for the classification task (in this case, binary classification)\n",
    "max_length = min(int(AutoModel.from_pretrained(model_name).config.max_position_embeddings), 512)  # Maximum length of the input sequences (truncation if larger than this). Set dynamically based on the chosen model.\n",
    "\n",
    "print(f\"Model: {model_name}, Max Length: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36bdee",
   "metadata": {},
   "source": [
    "Second, we tokenize the text and create the vocabulary IDs in order to create the inputs for the BERT-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bd2e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4725a008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: Faits: A. Le 28 octobre 2002 à 14 h.50, A._, domiciliée en France, circulait, au volant d'une voiture dont elle est détentrice, sur la route d'Hermance (GE) en direction de Corsier, après avoir quitté une place de stationnement. Alors qu'elle obliquait à gauche pour entrer dans le parking de la poste de Collonge-Bellerive en vue d'effectuer un demi-tour, l'arrière gauche de sa voiture a été heurté par un scooter de marque Honda Fes piloté par son détenteur, B._. Ce dernier a été grièvement blessé. Au lieu de l'accident, la vitesse maximale autorisée était de 50 km/h. A l'endroit où l'automobiliste a obliqué à gauche - manoeuvre qui était permise -, la ligne centrale est pointillée, alors qu'elle est pleine avant et après cet endroit. Le scooter a laissé une trace de freinage de 9,7 mètres qui commence sur la ligne de sécurité et évolue vers la gauche. La conductrice de la voiture a reconnu qu'elle n'avait pas regardé derrière elle avant d'obliquer. La police a infligé une amende à B._ pour avoir perdu la maîtrise de son véhicule. Le rapport de police a conclu qu'il avait perdu la maîtrise de son engin, ayant dû freiner en urgence et n'ayant pas remarqué que le véhicule qui le précédait avait enclenché son clignotant. B._ a déposé plainte pénale contre A._ pour lésions corporelles par négligence et dommages à la propriété. Une expertise judiciaire a été ordonnée. En substance, l'expert est parvenu à la conclusion que le motocycliste roulait à une vitesse inadaptée, comprise entre 52 et 58 km/h., et qu'il avait entrepris de dépasser la voiture en franchissant la ligne de sécurité, sans remarquer que la conductrice avait enclenché son indicateur de direction à gauche. Même si la conductrice avait regardé dans son rétroviseur, elle n'aurait pas eu de raison de renoncer à sa manoeuvre - étant rappelé qu'elle ne peut pas regarder constamment derrière elle -, puisqu'elle n'avait pas à s'attendre à ce que le pilote du scooter dépasse une voiture qui avait manifesté son intention d'obliquer à gauche. Entendue par le juge d'instruction, la conductrice a déclaré qu'elle n'était pas sûre à 100 % d'avoir enclenché son clignotant. L'expert a relevé qu'il ne pouvait pas se prononcer sur cette question. Estimant qu'il n'était pas possible d'établir les éléments constitutifs d'une infraction, le juge d'instruction a refusé d'inculper la conductrice, décision qui fut confirmée en dernier ressort par le Tribunal fédéral le 18 juin 2004 (arrêt 6P.69/2004 et 6S.188/2004). B. Le 7 août 2009, B._ a ouvert action en paiement devant les autorités genevoises contre la détentrice de la voiture A._, Y._, assureur qui couvre la responsabilité civile de la détentrice, et le Bureau national d'assurance, en tant qu'il couvre les véhicules étrangers. Il a conclu à ce que les défendeurs soient condamnés solidairement à lui payer la somme de 3'994'846 fr.66 avec intérêts à 5% dès le 28 octobre 2002 à titre de dommages-intérêts et réparation du tort moral. Les défendeurs se sont opposés à la demande en totalité. Le demandeur a produit une expertise privée, dont les conclusions divergent de celles de l'expertise recueillie dans la procédure pénale. Selon l'expert privé, il n'est pas exclu que le scooter ait roulé à la vitesse autorisée, soit 50 km/h., et on ne peut pas dire que cette vitesse était inadaptée; il est possible que le pilote du scooter ait eu l'intention de dépasser la voiture en restant à l'intérieur de sa voie de circulation, c'est-à-dire sans franchir la ligne de sécurité; son déplacement sur la gauche pourrait être dicté uniquement par une réaction instinctive d'évitement; si la conductrice avait regardé dans son rétroviseur avant d'obliquer, elle aurait perçu le danger en raison de la différence de vitesses. Le Tribunal de première instance de Genève a décidé de se prononcer d'abord sur \"le principe de la responsabilité\". Par jugement du 8 septembre 2010, il a constaté que les défendeurs étaient, sur le principe, civilement responsables du dommage subi par B._ et a réservé la suite de la procédure. Le premier juge a considéré que les faits établis ne permettaient pas de conclure que l'accident était dû à une faute grave et exclusive de B._, laquelle a interrompu le rapport de causalité. Statuant sur appel des défendeurs par arrêt du 18 mars 2011, la Chambre civile de la Cour de justice de Genève a confirmé le jugement attaqué. La cour cantonale est parvenue à la conclusion qu'aucune faute grave et exclusive n'avait été établie et que la suite de la procédure déterminerait le montant du dommage et sa répartition. C. Le Bureau national d'assurance, Y._ et A._ exercent un recours en matière civile au Tribunal fédéral contre l'arrêt précité. Invoquant l'arbitraire dans l'établissement des faits et dans l'application du droit de procédure cantonal, une violation du droit d'être entendu, ainsi qu'une violation des art. 53 CO, 8 CC, 34, 61, 90 de la Loi fédérale sur la circulation routière du 19 décembre 1958 (LCR; RS 741.01) et 3 de l'Ordonnance sur les règles de la circulation routière du 13 novembre 1962 (OCR; RS 741.11), ils concluent à l'annulation de l'arrêt attaqué et au constat qu'ils ne sont pas civilement responsables de l'accident. L'intimé propose le rejet du recours . \n",
      "Token IDs: [5, 9447, 10, 92, 114, 9, 54, 1170, 732, 5162, 15, 476, 616, 9, 1574, 7, 114, 9, 862, 7, 22180, 35, 22, 184, 7, 19784, 6093, 7, 36, 5345, 18, 11, 70, 864, 174, 109, 30, 22335, 5479, 7, 32, 13, 876, 18, 11, 12478, 824, 291, 38, 7525, 53, 22, 901, 8, 2618, 10, 946, 7, 182, 190, 5003, 28, 218, 8, 8177, 9, 574, 46, 11, 144, 5636, 1187, 1358, 199, 15, 953, 24, 3305, 29, 16, 3029, 8, 13, 1337, 8, 2875, 8097, 35, 26, 385, 144, 7820, 22, 477, 18, 11, 6331, 23, 1644, 26, 4416, 7, 17, 11, 3218, 953, 8, 77, 864, 33, 101, 15632, 141, 37, 23, 16405, 8, 587, 17979, 5012, 10, 23187, 37, 58, 15084, 7, 261, 9, 862, 9, 148, 348, 33, 101, 21, 7162, 6300, 131, 7673, 9, 277, 322, 8, 17, 11, 7346, 7, 13, 1504, 5944, 13101, 149, 8, 712, 879, 122, 133, 9, 114, 17, 11, 3390, 147, 17, 11, 30228, 33, 5636, 219, 13497, 15, 953, 67, 20985, 31, 149, 994, 35, 67, 7, 13, 284, 2405, 30, 299, 13735, 7, 183, 46, 11, 144, 30, 1430, 178, 14, 182, 280, 1643, 9, 54, 16405, 33, 2310, 28, 5167, 8, 17186, 8, 21, 30699, 1436, 31, 1246, 32, 13, 284, 8, 548, 14, 8600, 224, 13, 953, 9, 61, 1113, 19336, 8, 13, 864, 33, 3224, 46, 11, 144, 49, 11, 687, 34, 8031, 1298, 109, 178, 18, 11, 3925, 4147, 81, 9, 61, 1642, 33, 22040, 28, 13302, 15, 261, 9, 862, 24, 190, 1576, 13, 3135, 8, 58, 1690, 9, 54, 459, 8, 1642, 33, 6048, 46, 11, 62, 171, 1576, 13, 3135, 8, 58, 22, 5185, 7, 634, 1556, 25265, 22, 7248, 14, 49, 11, 3276, 34, 5002, 27, 16, 1690, 31, 16, 790, 2162, 204, 199, 171, 22, 7566, 90, 2264, 58, 28094, 9, 261, 9, 862, 33, 7145, 6757, 8790, 192, 114, 9, 862, 24, 19413, 13605, 10, 37, 27016, 14, 5176, 15, 13, 1772, 9, 180, 5741, 4742, 33, 101, 10826, 35, 9, 107, 9152, 7, 17, 11, 13636, 30, 12047, 15, 13, 5610, 27, 16, 3209, 9737, 905, 18233, 199, 15, 28, 1504, 27285, 35, 7, 13060, 128, 5501, 14, 6928, 879, 122, 133, 9, 7, 14, 46, 11, 62, 171, 15819, 8, 5581, 13, 864, 22, 9009, 2927, 13, 284, 8, 548, 7, 112, 5132, 27, 13, 1113, 19336, 171, 22, 7566, 90, 2264, 58, 18354, 8, 901, 15, 953, 9, 1410, 86, 13, 1113, 19336, 171, 8031, 29, 58, 27494, 7, 109, 49, 11, 3187, 34, 331, 8, 539, 8, 12620, 15, 77, 20985, 67, 677, 7362, 46, 11, 144, 45, 104, 34, 1483, 7062, 1298, 109, 67, 7, 1957, 11, 144, 49, 11, 687, 34, 15, 52, 11, 5627, 15, 44, 27, 16, 4479, 25, 16405, 6814, 28, 864, 31, 171, 17217, 58, 7611, 18, 11, 3925, 4147, 81, 15, 953, 9, 107, 23781, 35, 37, 16, 2204, 18, 11, 8572, 7, 13, 1113, 19336, 33, 1961, 46, 11, 144, 49, 11, 230, 34, 5189, 6]\n",
      "Length: 512\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sample_text = train_df.select('text')['text'][0]  # Select the text of the first sample\n",
    "\n",
    "# Tokenize the sample text considering the maximum length\n",
    "tokens = tokenizer(\n",
    "    sample_text, \n",
    "    max_length=max_length,  # Truncate to the maximum length of the model \n",
    "    truncation=True, \n",
    "    padding='max_length'  # Pad to the maximum length\n",
    ")\n",
    "\n",
    "# Display input IDs (tokenized & padded)\n",
    "print(f\"Sample text: {sample_text}\")\n",
    "print(\"Token IDs:\", tokens[\"input_ids\"])\n",
    "print(\"Length:\", len(tokens[\"input_ids\"]))\n",
    "print(\"Attention Mask:\", tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab84366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c5dad3951f4053821eec595ee12599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50df04feb0c443f99ba8d62c621ec0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First example from the training set:\n",
      "{'labels': tensor(1), 'input_ids': tensor([    5,  9447,    10,    43,  9447,    10,    43,   114,     9,   502,\n",
      "          124,   108,   732,  7416,    36,   181,   667,  9474,   275,     9,\n",
      "          862,    33, 11125,    20, 19377,     8,  4103,    31,   111,    96,\n",
      "          101, 20796,    37,    13, 12088,     8,  4103,    25,  8259, 12254,\n",
      "          537,  2143,   156,  9311,    38,   244,    26,   856,    92,    13,\n",
      "         5454,     8,  4103,    53,    42,   340,  5981,  9637,    36,   205,\n",
      "          667,  7416,     9,    54,   409,   652,  5985,     7,    51,   171,\n",
      "         7145,    28,   400,     8,  2734,   843,     8,    17,    11,  8321,\n",
      "        27333,     8,    17,    11,  2049,    26, 21556,    38,   244,    26,\n",
      "          856,    92,    17,    11,  7720,    21,  7771,   120,   211,    23,\n",
      "          327,     8,  1141,    25,  1157,   732,  9474,  4109,   666,    29,\n",
      "           28,  1141,    25,   135,   871, 10801,    17,    11,  7720,    21,\n",
      "         7771,    33,  3122,    36,  2992,  2037,    46,    11,    62,  9505,\n",
      "          199,     8,   111, 22314,  6722,    28,  1644,    26,  3587,    35,\n",
      "           18,    11, 21556,    15,   350,    25,   118,   652,  5985,     7,\n",
      "         5508,    32,    23,  5255,    18,    11, 21556,     8,  5815, 11091,\n",
      "          372,  1141,    25,   653,   925, 10801,    13,  5454,     8,  4103,\n",
      "           33, 17556,     8,   275,     9,   862,    13, 18466,    18,    11,\n",
      "           70,  2124,     8,  9254,   205,  4371,  2542,     9,   325,     7,\n",
      "          174,  1290,  5678,   367,  2542,     9,   530,   902, 21027,    10,\n",
      "           42,    19,  2734,     8,    17,    11,  2049,    26, 21556,    14,\n",
      "         4868,   205,  2561,  2542,     9,   325,  7433,   111,    98, 11071,\n",
      "           10,    37,    17,    11, 21879,   902,     9,   148,  1520,  2174,\n",
      "          199,    68,  2734,    27,   330,    26,   244,   171,     7,   395,\n",
      "          109,     7, 15700,    10,    22,   237,     8,    17,    11,  2049,\n",
      "           26,   751,  2908,   215,   491,    25,   205,   667,  7416,    36,\n",
      "          181,   667,  2477,     9,    54,  1520,  2907,    15, 18466,   149,\n",
      "        28302,    36,  5255,     8,    17,    11, 20800,     8,  5112,  7793,\n",
      "           37,    17,    11,  2049,    26, 21556,    38,  4662, 19529,    54,\n",
      "           93,   209,     7,   109,    33,  9385,     8,    13,  5454,     8,\n",
      "        13724,   114, 19008,     8,    13,  2157,  4364,    20,  8601,    13,\n",
      "        13724,    25,  1520,     8,  1290,  5678,   367,  2542,     9,    24,\n",
      "           13,   782,    25,   205,   667,  7416,    36,   181,   667,  2477,\n",
      "            9,   372,  1141,    25,   653,   925, 10801,    13,  5454,     8,\n",
      "         4103,    33, 17556,     8,   275,     9,   862,    13, 18466,    18,\n",
      "           11,    70,  2124,     8,  9254,   205,  4371,  2542,     9,   325,\n",
      "            7,   174,  1290,  5678,   367,  2542,     9,   530,   902, 21027,\n",
      "           10,    42,    19,  2734,     8,    17,    11,  2049,    26, 21556,\n",
      "           14,  4868,   205,  2561,  2542,     9,   325,  7433,   111,    98,\n",
      "        11071,    10,    37,    17,    11, 21879,   902,     9,   148,  1520,\n",
      "         2174,   199,    68,  2734,    27,   330,    26,   244,   171,     7,\n",
      "          395,   109,     7, 15700,    10,    22,   237,     8,    17,    11,\n",
      "         2049,    26,   751,  2908,   215,   491,    25,   205,   667,  7416,\n",
      "           36,   181,   667,  2477,     9,    54,  1520,  2907,    15, 18466,\n",
      "          149, 28302,    36,  5255,     8,    17,    11, 20800,     8,  5112,\n",
      "         7793,    37,    17,    11,  2049,    26, 21556,    38,  4662, 19529,\n",
      "           54,    93,   209,     7,   109,    33,  9385,     8,    13,  5454,\n",
      "            8, 13724,   114, 19008,     8,    13,  2157,  4364,    20,  8601,\n",
      "           13, 13724,    25,  1520,     8,  1290,  5678,   367,  2542,     9,\n",
      "           24,    13,   782,    25,   205,   667,  7416,    36,   181,   667,\n",
      "         2477,     9,   261,     9,   261,     9,    55,    54,   301,   667,\n",
      "        10801,     6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "# We convert the Polars Data Frame to an arrow Dataset and get a sample of the training data\n",
    "if 'labels' in train_df_fr.columns:\n",
    "    train_df_fr = train_df_fr.rename({'labels': 'label'})  # Rename 'labels' to 'label' for compatibility with sampling function\n",
    "train_set_fr = sample_balanced_dataset(train_df_fr, 32, seed)\n",
    "train_set_fr = train_set_fr.rename_column('label', 'labels')  # Rename the label column again to 'labels' for compatibility with Hugging Face Trainer\n",
    "# train_set_fr = Dataset.from_polars(train_df_fr.select(['text', 'labels']).sample(n=32, shuffle=True, seed=seed))\n",
    "val_set_fr = Dataset.from_polars(val_df_fr.select(['text', 'labels']))\n",
    "\n",
    "# Now, we actually tokenize the datasets\n",
    "train_set_fr = train_set_fr.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "val_set_fr = val_set_fr.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Convert the data in the datasets to PyTorch tensors\n",
    "train_set_fr.set_format(type='torch')\n",
    "val_set_fr.set_format(type='torch')\n",
    "\n",
    "# Display the first example from the training set\n",
    "print(\"\\nFirst example from the training set:\")\n",
    "print(train_set_fr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97165b0",
   "metadata": {},
   "source": [
    "The transformation has been executed as expected: we now have all of the features (input IDs) and labels saved as tensors. Remember that the attention mask explicits which tokens are actually real (1) and which are padded (0). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50eaa9",
   "metadata": {},
   "source": [
    "Third, we initialize the `AutoModelForSequenceClassification` class from Hugging Face, which automatically handles the creation of a classification layer on top of the embedding layers of the BERT-based model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e81154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at almanach/camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe40fe9",
   "metadata": {},
   "source": [
    "Fourth, we configure the relevant training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a3d3289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory for the model: models/part_2/a/cls_fine_tuning_camembert-base\n"
     ]
    }
   ],
   "source": [
    "san_model_name = model_name.split(sep='/')[-1]  # Sanitize model name for file path, keep only the last part\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir = os.path.join('models', 'part_2', 'a', f'cls_fine_tuning_{san_model_name}'),\n",
    "    eval_strategy = \"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy = \"epoch\",  # Save the model at the end of each epoch\n",
    "    logging_strategy = \"steps\",  # Log training progress every logging_steps\n",
    "    logging_steps = 50,\n",
    "    learning_rate = 2e-5,  # Learning rate for fine-tuning the language model\n",
    "    per_device_train_batch_size = 16,  # Batch size for training\n",
    "    per_device_eval_batch_size  = 16,  # Batch size for evaluation\n",
    "    num_train_epochs = 20,  # Maximum number of epochs for training (unless early stopping is triggered)\n",
    "    weight_decay = 0.01,  # Weight decay for regularization\n",
    "    load_best_model_at_end = True,  # Load the best model at the end of training based on evaluation loss\n",
    "    metric_for_best_model = \"eval_loss\",  # Metric to use for determining the best model\n",
    "    save_total_limit = 2,  # Limit the total number of saved models to the best 2 models\n",
    "    seed = seed,  # Set random seed for reproducibility\n",
    "    report_to = \"none\",  # Disable wandb\n",
    "    fp16 = True, # Enables mixed precision training (compatible on some GPUs, but not all). Set to True if you have a compatible GPU.\n",
    "    gradient_accumulation_steps = 2  # Gradient accumulation steps to effectively increase the batch size without increasing memory usage\n",
    ")\n",
    "\n",
    "print(\"Output directory for the model:\", train_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c1f953",
   "metadata": {},
   "source": [
    "Finally, we initialize the `Trainer` class, fine-tune the embedding model and train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f5235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/20 04:57 < 01:54, 0.04 it/s, Epoch 15/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.697425</td>\n",
       "      <td>0.276575</td>\n",
       "      <td>0.339333</td>\n",
       "      <td>0.209854</td>\n",
       "      <td>0.885978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696253</td>\n",
       "      <td>0.335703</td>\n",
       "      <td>0.332901</td>\n",
       "      <td>0.210851</td>\n",
       "      <td>0.790447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698141</td>\n",
       "      <td>0.267528</td>\n",
       "      <td>0.341563</td>\n",
       "      <td>0.210451</td>\n",
       "      <td>0.906009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698896</td>\n",
       "      <td>0.256866</td>\n",
       "      <td>0.340218</td>\n",
       "      <td>0.209024</td>\n",
       "      <td>0.913713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.697306</td>\n",
       "      <td>0.338934</td>\n",
       "      <td>0.336576</td>\n",
       "      <td>0.213142</td>\n",
       "      <td>0.799692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694188</td>\n",
       "      <td>0.490792</td>\n",
       "      <td>0.306948</td>\n",
       "      <td>0.214769</td>\n",
       "      <td>0.537750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690351</td>\n",
       "      <td>0.621002</td>\n",
       "      <td>0.262728</td>\n",
       "      <td>0.221868</td>\n",
       "      <td>0.322034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688712</td>\n",
       "      <td>0.649435</td>\n",
       "      <td>0.252240</td>\n",
       "      <td>0.228180</td>\n",
       "      <td>0.281972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.687847</td>\n",
       "      <td>0.655574</td>\n",
       "      <td>0.242898</td>\n",
       "      <td>0.225296</td>\n",
       "      <td>0.263482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686955</td>\n",
       "      <td>0.660420</td>\n",
       "      <td>0.240058</td>\n",
       "      <td>0.226158</td>\n",
       "      <td>0.255778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.687761</td>\n",
       "      <td>0.646527</td>\n",
       "      <td>0.252732</td>\n",
       "      <td>0.226994</td>\n",
       "      <td>0.285054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688309</td>\n",
       "      <td>0.634895</td>\n",
       "      <td>0.267185</td>\n",
       "      <td>0.230683</td>\n",
       "      <td>0.317411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689218</td>\n",
       "      <td>0.617447</td>\n",
       "      <td>0.279805</td>\n",
       "      <td>0.231156</td>\n",
       "      <td>0.354391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689515</td>\n",
       "      <td>0.607108</td>\n",
       "      <td>0.282172</td>\n",
       "      <td>0.228708</td>\n",
       "      <td>0.368259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689564</td>\n",
       "      <td>0.605170</td>\n",
       "      <td>0.286215</td>\n",
       "      <td>0.230480</td>\n",
       "      <td>0.377504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_set_fr,\n",
    "    eval_dataset=val_set_fr.shuffle(seed=seed),  # Shuffle the validation set for evaluation\n",
    "    compute_metrics=lambda p: {  # Return a dictionary of metrics\n",
    "        \"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),  # Compare the true labels with the predicted labels, by selecting the index with the highest probability\n",
    "        \"f1\": f1_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "        \"precision\": precision_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "        \"recall\": recall_score(p.label_ids, np.argmax(p.predictions, axis=1))\n",
    "    },\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # Early stopping after 3 epochs without improvement\n",
    ")\n",
    "\n",
    "train_output = trainer.train()\n",
    "\n",
    "# Filter for epoch logs (those that have 'epoch' key)\n",
    "log_history = trainer.state.log_history\n",
    "epoch_logs = [log for log in log_history if 'epoch' in log]\n",
    "\n",
    "# Convert to Polars DataFrame\n",
    "results_df = pl.DataFrame(epoch_logs)\n",
    "\n",
    "# Save results in a Parquet file\n",
    "results_path = os.path.join('results', 'part_2', 'a', f'cls_fine_tuning_results_{san_model_name}.parquet')\n",
    "os.makedirs(os.path.dirname(results_path), exist_ok=True)  # Ensure the directory exists\n",
    "results_df.write_parquet(results_path)\n",
    "print(f'Results saved to: {results_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b317b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (16, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>eval_loss</th><th>eval_accuracy</th><th>eval_f1</th><th>eval_precision</th><th>eval_recall</th><th>eval_runtime</th><th>eval_samples_per_second</th><th>eval_steps_per_second</th><th>epoch</th><th>step</th><th>train_runtime</th><th>train_samples_per_second</th><th>train_steps_per_second</th><th>total_flos</th><th>train_loss</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.697425</td><td>0.276575</td><td>0.339333</td><td>0.209854</td><td>0.885978</td><td>17.2477</td><td>179.444</td><td>11.248</td><td>1.0</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.696253</td><td>0.335703</td><td>0.332901</td><td>0.210851</td><td>0.790447</td><td>17.599</td><td>175.862</td><td>11.023</td><td>2.0</td><td>2</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.698141</td><td>0.267528</td><td>0.341563</td><td>0.210451</td><td>0.906009</td><td>17.7029</td><td>174.83</td><td>10.959</td><td>3.0</td><td>3</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.698896</td><td>0.256866</td><td>0.340218</td><td>0.209024</td><td>0.913713</td><td>17.64</td><td>175.454</td><td>10.998</td><td>4.0</td><td>4</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.697306</td><td>0.338934</td><td>0.336576</td><td>0.213142</td><td>0.799692</td><td>17.6216</td><td>175.637</td><td>11.009</td><td>5.0</td><td>5</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.688309</td><td>0.634895</td><td>0.267185</td><td>0.230683</td><td>0.317411</td><td>17.5951</td><td>175.901</td><td>11.026</td><td>12.0</td><td>12</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.689218</td><td>0.617447</td><td>0.279805</td><td>0.231156</td><td>0.354391</td><td>17.6085</td><td>175.767</td><td>11.017</td><td>13.0</td><td>13</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.689515</td><td>0.607108</td><td>0.282172</td><td>0.228708</td><td>0.368259</td><td>17.6307</td><td>175.547</td><td>11.004</td><td>14.0</td><td>14</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.689564</td><td>0.60517</td><td>0.286215</td><td>0.23048</td><td>0.377504</td><td>17.6082</td><td>175.77</td><td>11.018</td><td>15.0</td><td>15</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>15.0</td><td>15</td><td>298.4083</td><td>2.145</td><td>0.067</td><td>1.2629e14</td><td>0.678275</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (16, 15)\n",
       "┌───────────┬───────────┬──────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ eval_loss ┆ eval_accu ┆ eval_f1  ┆ eval_prec ┆ … ┆ train_sam ┆ train_ste ┆ total_flo ┆ train_los │\n",
       "│ ---       ┆ racy      ┆ ---      ┆ ision     ┆   ┆ ples_per_ ┆ ps_per_se ┆ s         ┆ s         │\n",
       "│ f64       ┆ ---       ┆ f64      ┆ ---       ┆   ┆ second    ┆ cond      ┆ ---       ┆ ---       │\n",
       "│           ┆ f64       ┆          ┆ f64       ┆   ┆ ---       ┆ ---       ┆ f64       ┆ f64       │\n",
       "│           ┆           ┆          ┆           ┆   ┆ f64       ┆ f64       ┆           ┆           │\n",
       "╞═══════════╪═══════════╪══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 0.697425  ┆ 0.276575  ┆ 0.339333 ┆ 0.209854  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.696253  ┆ 0.335703  ┆ 0.332901 ┆ 0.210851  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.698141  ┆ 0.267528  ┆ 0.341563 ┆ 0.210451  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.698896  ┆ 0.256866  ┆ 0.340218 ┆ 0.209024  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.697306  ┆ 0.338934  ┆ 0.336576 ┆ 0.213142  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ …         ┆ …         ┆ …        ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 0.688309  ┆ 0.634895  ┆ 0.267185 ┆ 0.230683  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.689218  ┆ 0.617447  ┆ 0.279805 ┆ 0.231156  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.689515  ┆ 0.607108  ┆ 0.282172 ┆ 0.228708  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.689564  ┆ 0.60517   ┆ 0.286215 ┆ 0.23048   ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ null      ┆ null      ┆ null     ┆ null      ┆ … ┆ 2.145     ┆ 0.067     ┆ 1.2629e14 ┆ 0.678275  │\n",
       "└───────────┴───────────┴──────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best epoch metrics (lowest validation loss):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>epoch</th><th>eval_loss</th><th>eval_accuracy</th><th>eval_f1</th><th>eval_precision</th><th>eval_recall</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>10.0</td><td>0.686955</td><td>0.66042</td><td>0.240058</td><td>0.226158</td><td>0.255778</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 6)\n",
       "┌───────┬───────────┬───────────────┬──────────┬────────────────┬─────────────┐\n",
       "│ epoch ┆ eval_loss ┆ eval_accuracy ┆ eval_f1  ┆ eval_precision ┆ eval_recall │\n",
       "│ ---   ┆ ---       ┆ ---           ┆ ---      ┆ ---            ┆ ---         │\n",
       "│ f64   ┆ f64       ┆ f64           ┆ f64      ┆ f64            ┆ f64         │\n",
       "╞═══════╪═══════════╪═══════════════╪══════════╪════════════════╪═════════════╡\n",
       "│ 10.0  ┆ 0.686955  ┆ 0.66042       ┆ 0.240058 ┆ 0.226158       ┆ 0.255778    │\n",
       "└───────┴───────────┴───────────────┴──────────┴────────────────┴─────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the results DataFrame from the Parquet file\n",
    "results_path = os.path.join('results', 'part_2', 'a', f'cls_fine_tuning_results_camembert-base.parquet')\n",
    "results_df = pl.read_parquet(results_path)\n",
    "\n",
    "# Display the metrics DataFrame\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "display(results_df)\n",
    "\n",
    "# Print the metrics for the epoch with the lowest validation loss\n",
    "best_epoch = results_df.filter(pl.col('eval_loss') == results_df['eval_loss'].min()).select(['epoch', 'eval_loss', 'eval_accuracy', 'eval_f1', 'eval_precision', 'eval_recall'])\n",
    "print(\"\\nBest epoch metrics (lowest validation loss):\")\n",
    "display(best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c1740",
   "metadata": {},
   "source": [
    "### With JuriBERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cff8f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dascim/juribert-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: dascim/juribert-base, Max Length: 512\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dascim/juribert-base\"  # Path to the pre-trained model\n",
    "num_labels = 2  # Number of labels for the classification task (in this case, binary classification)\n",
    "max_length = min(int(AutoModel.from_pretrained(model_name).config.max_position_embeddings), 512)  # Maximum length of the input sequences (truncation if larger than this). Set dynamically based on the chosen model.\n",
    "\n",
    "print(f\"Model: {model_name}, Max Length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d43691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986e66db88e4c4bacb5b8fdc1763950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27dc8718a0647c5b9ce86f62bad90b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First example from the training set:\n",
      "{'labels': tensor(1), 'input_ids': tensor([    0,    42,   322,    87,   548,   225,    42,   322,    87,   548,\n",
      "          225,    37,    18,   225,    40,    89,   349,   279,   866,  2315,\n",
      "          315,   627,   837,  1823,    16,   225,    52,    18,    67,   270,\n",
      "         4158,   321,  2647,   265,  5875,   397,   596,   672,   438,  2312,\n",
      "          308,   277,   225,    39,  1188,   265,  5875,   300,  1850,  7193,\n",
      "         2680,  3163,   225,    55,    45,    38,   464,   311,    17,  3492,\n",
      "           30,   277,  1263,   265,  5875,    13,   628,   935,  9366,   315,\n",
      "          697,   837,  2315,    18,   225,    48,    73,  1076,   911,  2599,\n",
      "           16,   425,   557,  2926,   402,   552,   265,  2308,  1852,   265,\n",
      "          263,    11,    51,  2196, 20005,   325,   265,   263,    11,  1844,\n",
      "           17,  6574,   464,   311,    17,  3492,    30,   263,    11,  2337,\n",
      "          225,    37,    45,   652,   225,    40,   359,   342,  2170,   265,\n",
      "          564,   300,  1681,   866,  1823,    16,  4131,  2568,   389,   402,\n",
      "          564,   300,   520,   937,  1821,    16,   263,    11,  2337,   225,\n",
      "           37,    45,   270,  5161,   315,   361, 22462,   287,    11,   318,\n",
      "        14481,   265,   596,  6580,   402,  7826,    17,    86,   601,   261,\n",
      "           11,  6574,   295,  2761,   300,   346,   911,  2599,    16,  1954,\n",
      "          375,   342,  4713,   261,    11,  6574,   265,  6340, 11162,   225,\n",
      "           52,   280,   564,   300,   539,   886,  1821,    16,   277,  1263,\n",
      "          265,  5875,   270,  7161,   265,   225,    52,    18,    67,   277,\n",
      "         2907,   261,    11,   467,   621,   265,  9345, 17708,   540,    18,\n",
      "          543,    16,   647,  1611,  1043,  3828,   540,    18,  1123,  2718,\n",
      "         1810,   404,   628,   327,  2308,   265,   263,    11,  1844,    17,\n",
      "         6574,   302,  6827, 31380,   540,    18,   543,  3605,   596,   549,\n",
      "        12463,   308,   263,    11,  3343,  2718,    18,   225,    39,    73,\n",
      "          929,  6226,   430,  2308,   305,  1224,    17,   311,   557,    16,\n",
      "          971,   576,    16,  6342,   309,  4691,   265,   263,    11,  1844,\n",
      "           17,   411,   727,  4240,   300,   697,   837,  2315,   315,   627,\n",
      "          837,  1823,    18,   225,    48,    73,   929,  2159,   295,  2907,\n",
      "          573, 13056,   315,  4713,   265,   263,    11,  3762,   265,  8443,\n",
      "         1613,   308,   263,    11,  1844,    17,  6574,   464,  4462,  2281,\n",
      "          652,   225,    48,    73,   639,  1139,    16,   576,   270,  6234,\n",
      "          265,   277,  1263,   265,  3908,   225,    37,    58,    55,   265,\n",
      "          277,   225,    55,  1230,   369,  6517,   321, 12989,   277,  3908,\n",
      "          300,   929,   265,  1611,  1043,  3828,   540,    18,   352,   277,\n",
      "         1447,   300,   697,   837,  2315,   315,   627,   837,  1823,    18,\n",
      "          225,    52,   280,   564,   300,   539,   886,  1821,    16,   277,\n",
      "         1263,   265,  5875,   270,  7161,   265,   225,    52,    18,    67,\n",
      "          277,  2907,   261,    11,   467,   621,   265,  9345, 17708,   540,\n",
      "           18,   543,    16,   647,  1611,  1043,  3828,   540,    18,  1123,\n",
      "         2718,  1810,   404,   628,   327,  2308,   265,   263,    11,  1844,\n",
      "           17,  6574,   302,  6827, 31380,   540,    18,   543,  3605,   596,\n",
      "          549, 12463,   308,   263,    11,  3343,  2718,    18,   225,    39,\n",
      "           73,   929,  6226,   430,  2308,   305,  1224,    17,   311,   557,\n",
      "           16,   971,   576,    16,  6342,   309,  4691,   265,   263,    11,\n",
      "         1844,    17,   411,   727,  4240,   300,   697,   837,  2315,   315,\n",
      "          627,   837,  1823,    18,   225,    48,    73,   929,  2159,   295,\n",
      "         2907,   573, 13056,   315,  4713,   265,   263,    11,  3762,   265,\n",
      "         8443,  1613,   308,   263,    11,  1844,    17,  6574,   464,  4462,\n",
      "         2281,   652,   225,    48,    73,   639,  1139,    16,   576,   270,\n",
      "         6234,   265,   277,  1263,   265,  3908,   225,    37,    58,    55,\n",
      "          265,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dascim/juribert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory for the model: models/part_2/a/cls_fine_tuning_juribert-base\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "# We convert the Polars Data Frame to an arrow Dataset and get a sample of the training data\n",
    "if 'labels' in train_df_fr.columns:\n",
    "    train_df_fr = train_df_fr.rename({'labels': 'label'})  # Rename 'labels' to 'label' for compatibility with sampling function\n",
    "train_set_fr = sample_balanced_dataset(train_df_fr, 32, seed)\n",
    "train_set_fr = train_set_fr.rename_column('label', 'labels')  # Rename the label column again to 'labels' for compatibility with Hugging Face Trainer\n",
    "# train_set_fr = Dataset.from_polars(train_df_fr.select(['text', 'labels']).sample(n=32, shuffle=True, seed=seed))\n",
    "val_set_fr = Dataset.from_polars(val_df_fr.select(['text', 'labels']))\n",
    "\n",
    "# Now, we actually tokenize the datasets\n",
    "train_set_fr = train_set_fr.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "val_set_fr = val_set_fr.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Convert the data in the datasets to PyTorch tensors\n",
    "train_set_fr.set_format(type='torch')\n",
    "val_set_fr.set_format(type='torch')\n",
    "\n",
    "# Display the first example from the training set\n",
    "print(\"\\nFirst example from the training set:\")\n",
    "print(train_set_fr[0])\n",
    "\n",
    "# Load the model for fine-tuning\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Set training arguments\n",
    "san_model_name = model_name.split(sep='/')[-1]  # Sanitize model name for file path, keep only the last part\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir = os.path.join('models', 'part_2', 'a', f'cls_fine_tuning_{san_model_name}'),\n",
    "    eval_strategy = \"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy = \"epoch\",  # Save the model at the end of each epoch\n",
    "    logging_strategy = \"steps\",  # Log training progress every logging_steps\n",
    "    logging_steps = 50,\n",
    "    learning_rate = 2e-5,  # Learning rate for fine-tuning the language model\n",
    "    per_device_train_batch_size = 16,  # Batch size for training\n",
    "    per_device_eval_batch_size  = 16,  # Batch size for evaluation\n",
    "    num_train_epochs = 20,  # Maximum number of epochs for training (unless early stopping is triggered)\n",
    "    weight_decay = 0.01,  # Weight decay for regularization\n",
    "    load_best_model_at_end = True,  # Load the best model at the end of training based on evaluation loss\n",
    "    metric_for_best_model = \"eval_loss\",  # Metric to use for determining the best model\n",
    "    save_total_limit = 2,  # Limit the total number of saved models to the best 2 models\n",
    "    seed = seed,  # Set random seed for reproducibility\n",
    "    report_to = \"none\",  # Disable wandb\n",
    "    fp16 = True, # Enables mixed precision training (compatible on some GPUs, but not all). Set to True if you have a compatible GPU.\n",
    "    gradient_accumulation_steps = 2  # Gradient accumulation steps to effectively increase the batch size without increasing memory usage\n",
    ")\n",
    "\n",
    "print(\"Output directory for the model:\", train_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6591d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/20 01:18 < 04:35, 0.05 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.675423</td>\n",
       "      <td>0.608724</td>\n",
       "      <td>0.253851</td>\n",
       "      <td>0.211499</td>\n",
       "      <td>0.317411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.644028</td>\n",
       "      <td>0.689499</td>\n",
       "      <td>0.215510</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.203390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.639690</td>\n",
       "      <td>0.685299</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.229617</td>\n",
       "      <td>0.212635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.645959</td>\n",
       "      <td>0.673667</td>\n",
       "      <td>0.237160</td>\n",
       "      <td>0.232593</td>\n",
       "      <td>0.241911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/194 00:13 < 00:03, 11.04 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     args=train_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     callbacks=[EarlyStoppingCallback(early_stopping_patience=\u001b[32m5\u001b[39m)]  \u001b[38;5;66;03m# Early stopping after 3 epochs without improvement\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m train_output = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2656\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2653\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2655\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2656\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2661\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2662\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/transformers/trainer.py:3095\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3093\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3095\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3096\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3098\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/transformers/trainer.py:3044\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3045\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3047\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/transformers/trainer.py:4173\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4170\u001b[39m start_time = time.time()\n\u001b[32m   4172\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4173\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4174\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4176\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4183\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/transformers/trainer.py:4390\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4388\u001b[39m     labels = \u001b[38;5;28mself\u001b[39m.accelerator.pad_across_processes(labels, dim=\u001b[32m1\u001b[39m, pad_index=-\u001b[32m100\u001b[39m)\n\u001b[32m   4389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4390\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4391\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.preprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4392\u001b[39m         logits = \u001b[38;5;28mself\u001b[39m.preprocess_logits_for_metrics(logits, labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/accelerate/accelerator.py:2832\u001b[39m, in \u001b[36mAccelerator.pad_across_processes\u001b[39m\u001b[34m(self, tensor, dim, pad_index, pad_first)\u001b[39m\n\u001b[32m   2799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim=\u001b[32m0\u001b[39m, pad_index=\u001b[32m0\u001b[39m, pad_first=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   2800\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2801\u001b[39m \u001b[33;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[32m   2802\u001b[39m \u001b[33;03m    they can safely be gathered.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2830\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m   2831\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/accelerate/utils/operations.py:407\u001b[39m, in \u001b[36mchained_operation.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    409\u001b[39m         operation = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction.\u001b[34m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/accelerate/utils/operations.py:677\u001b[39m, in \u001b[36mpad_across_processes\u001b[39m\u001b[34m(tensor, dim, pad_index, pad_first)\u001b[39m\n\u001b[32m    674\u001b[39m     new_tensor[indices] = tensor\n\u001b[32m    675\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_first\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/accelerate/utils/operations.py:126\u001b[39m, in \u001b[36mrecursively_apply\u001b[39m\u001b[34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[32m    118\u001b[39m         {\n\u001b[32m    119\u001b[39m             k: recursively_apply(\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m         }\n\u001b[32m    124\u001b[39m     )\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    129\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. Only nested list/tuple/dicts of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` should be passed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/accelerate/utils/operations.py:657\u001b[39m, in \u001b[36mpad_across_processes.<locals>._pad_across_processes\u001b[39m\u001b[34m(tensor, dim, pad_index, pad_first)\u001b[39m\n\u001b[32m    654\u001b[39m     dim += \u001b[38;5;28mlen\u001b[39m(tensor.shape)\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m size = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    658\u001b[39m sizes = gather(size).cpu()\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_set_fr,\n",
    "    eval_dataset=val_set_fr.shuffle(seed=seed),  # Shuffle the validation set for evaluation\n",
    "    compute_metrics=lambda p: {  # Return a dictionary of metrics\n",
    "        \"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),  # Compare the true labels with the predicted labels, by selecting the index with the highest probability\n",
    "        \"f1\": f1_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "        \"precision\": precision_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "        \"recall\": recall_score(p.label_ids, np.argmax(p.predictions, axis=1))\n",
    "    },\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # Early stopping after 3 epochs without improvement\n",
    ")\n",
    "\n",
    "train_output = trainer.train()\n",
    "\n",
    "# Filter for epoch logs (those that have 'epoch' key)\n",
    "log_history = trainer.state.log_history\n",
    "epoch_logs = [log for log in log_history if 'epoch' in log]\n",
    "\n",
    "# Convert to Polars DataFrame\n",
    "results_df = pl.DataFrame(epoch_logs)\n",
    "\n",
    "# Save results in a Parquet file\n",
    "results_path = os.path.join('results', 'part_2', 'a', f'cls_fine_tuning_results_{san_model_name}.parquet')\n",
    "os.makedirs(os.path.dirname(results_path), exist_ok=True)  # Ensure the directory exists\n",
    "results_df.write_parquet(results_path)\n",
    "print(f'Results saved to: {results_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f7931aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>eval_loss</th><th>eval_accuracy</th><th>eval_f1</th><th>eval_precision</th><th>eval_recall</th><th>eval_runtime</th><th>eval_samples_per_second</th><th>eval_steps_per_second</th><th>epoch</th><th>step</th><th>train_runtime</th><th>train_samples_per_second</th><th>train_steps_per_second</th><th>total_flos</th><th>train_loss</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.793118</td><td>0.231018</td><td>0.344714</td><td>0.209856</td><td>0.964561</td><td>17.0745</td><td>181.265</td><td>11.362</td><td>1.0</td><td>1</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.683671</td><td>0.564459</td><td>0.263388</td><td>0.204064</td><td>0.371341</td><td>17.4299</td><td>177.568</td><td>11.13</td><td>2.0</td><td>2</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.641419</td><td>0.71664</td><td>0.125623</td><td>0.177966</td><td>0.097072</td><td>17.6116</td><td>175.736</td><td>11.015</td><td>3.0</td><td>3</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.632946</td><td>0.719548</td><td>0.143984</td><td>0.2</td><td>0.112481</td><td>17.6293</td><td>175.56</td><td>11.004</td><td>4.0</td><td>4</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.640205</td><td>0.685299</td><td>0.193709</td><td>0.209302</td><td>0.180277</td><td>17.5936</td><td>175.916</td><td>11.027</td><td>5.0</td><td>5</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.657755</td><td>0.632633</td><td>0.239465</td><td>0.211584</td><td>0.275809</td><td>17.5726</td><td>176.126</td><td>11.04</td><td>6.0</td><td>6</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.681419</td><td>0.580614</td><td>0.277283</td><td>0.217088</td><td>0.383667</td><td>17.6266</td><td>175.587</td><td>11.006</td><td>7.0</td><td>7</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.703745</td><td>0.536349</td><td>0.309764</td><td>0.225175</td><td>0.496148</td><td>17.5646</td><td>176.207</td><td>11.045</td><td>8.0</td><td>8</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>0.722242</td><td>0.504685</td><td>0.318364</td><td>0.22375</td><td>0.551618</td><td>17.5546</td><td>176.307</td><td>11.051</td><td>9.0</td><td>9</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>9.0</td><td>9</td><td>177.8374</td><td>3.599</td><td>0.112</td><td>7.5776e13</td><td>0.620199</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 15)\n",
       "┌───────────┬───────────┬──────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ eval_loss ┆ eval_accu ┆ eval_f1  ┆ eval_prec ┆ … ┆ train_sam ┆ train_ste ┆ total_flo ┆ train_los │\n",
       "│ ---       ┆ racy      ┆ ---      ┆ ision     ┆   ┆ ples_per_ ┆ ps_per_se ┆ s         ┆ s         │\n",
       "│ f64       ┆ ---       ┆ f64      ┆ ---       ┆   ┆ second    ┆ cond      ┆ ---       ┆ ---       │\n",
       "│           ┆ f64       ┆          ┆ f64       ┆   ┆ ---       ┆ ---       ┆ f64       ┆ f64       │\n",
       "│           ┆           ┆          ┆           ┆   ┆ f64       ┆ f64       ┆           ┆           │\n",
       "╞═══════════╪═══════════╪══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 0.793118  ┆ 0.231018  ┆ 0.344714 ┆ 0.209856  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.683671  ┆ 0.564459  ┆ 0.263388 ┆ 0.204064  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.641419  ┆ 0.71664   ┆ 0.125623 ┆ 0.177966  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.632946  ┆ 0.719548  ┆ 0.143984 ┆ 0.2       ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.640205  ┆ 0.685299  ┆ 0.193709 ┆ 0.209302  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.657755  ┆ 0.632633  ┆ 0.239465 ┆ 0.211584  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.681419  ┆ 0.580614  ┆ 0.277283 ┆ 0.217088  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.703745  ┆ 0.536349  ┆ 0.309764 ┆ 0.225175  ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ 0.722242  ┆ 0.504685  ┆ 0.318364 ┆ 0.22375   ┆ … ┆ null      ┆ null      ┆ null      ┆ null      │\n",
       "│ null      ┆ null      ┆ null     ┆ null      ┆ … ┆ 3.599     ┆ 0.112     ┆ 7.5776e13 ┆ 0.620199  │\n",
       "└───────────┴───────────┴──────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best epoch metrics (lowest validation loss):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>epoch</th><th>eval_loss</th><th>eval_accuracy</th><th>eval_f1</th><th>eval_precision</th><th>eval_recall</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>4.0</td><td>0.632946</td><td>0.719548</td><td>0.143984</td><td>0.2</td><td>0.112481</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 6)\n",
       "┌───────┬───────────┬───────────────┬──────────┬────────────────┬─────────────┐\n",
       "│ epoch ┆ eval_loss ┆ eval_accuracy ┆ eval_f1  ┆ eval_precision ┆ eval_recall │\n",
       "│ ---   ┆ ---       ┆ ---           ┆ ---      ┆ ---            ┆ ---         │\n",
       "│ f64   ┆ f64       ┆ f64           ┆ f64      ┆ f64            ┆ f64         │\n",
       "╞═══════╪═══════════╪═══════════════╪══════════╪════════════════╪═════════════╡\n",
       "│ 4.0   ┆ 0.632946  ┆ 0.719548      ┆ 0.143984 ┆ 0.2            ┆ 0.112481    │\n",
       "└───────┴───────────┴───────────────┴──────────┴────────────────┴─────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the results DataFrame from the Parquet file\n",
    "results_path = os.path.join('results', 'part_2', 'a', f'cls_fine_tuning_results_juribert-base.parquet')\n",
    "results_df = pl.read_parquet(results_path)\n",
    "\n",
    "# Display the metrics DataFrame\n",
    "print(\"\\nMetrics DataFrame:\")\n",
    "display(results_df)\n",
    "\n",
    "# Print the metrics for the epoch with the lowest validation loss\n",
    "best_epoch = results_df.filter(pl.col('eval_loss') == results_df['eval_loss'].min()).select(['epoch', 'eval_loss', 'eval_accuracy', 'eval_f1', 'eval_precision', 'eval_recall'])\n",
    "print(\"\\nBest epoch metrics (lowest validation loss):\")\n",
    "display(best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaca434",
   "metadata": {},
   "source": [
    "# 2. Dataset Augmentation\n",
    "\n",
    "Outline of the intermediate tasks: We want a fully automated pipeline. A good candidate is Easy Data Augmentation (EDA) or back-translation via open‐source MT models.\n",
    "1. Choose Technique(s)\n",
    "   - Easy Data Augmentation techniques (random insertion, deletion, swap).\n",
    "   - Random synonym substitution (WordNet or fastText).\n",
    "   - Back-translation: FR → EN → FR using MarianMT or opus-MT.\n",
    "2. Implement & Generate\n",
    "   - For each of the 32 labeled examples, generate k augmented pseudo-examples (e.g. k=5).\n",
    "   - Deduplicate and filter (e.g. reject if new text <50% overlap).\n",
    "3. Merge & Re-split\n",
    "   - Combine original 32 + synthetic N = 32×k examples.\n",
    "   - Re-run the same CV split strategy, ensuring augmented copies of a given original stay in the same fold.\n",
    "4. Re-train BERT\n",
    "   - Exactly the same hyperparams as in (a).\n",
    "   - Track performance uplift vs. the baseline.\n",
    "5. Analysis\n",
    "   - Compare metrics: ΔAccuracy, ΔF1.\n",
    "   - Ablation: EDA vs. back-translation vs. combined.\n",
    "   - Qualitative: inspect a few synthetic samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ccf8a",
   "metadata": {},
   "source": [
    "# 3. Zero-Shot Learning with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb57e3",
   "metadata": {},
   "source": [
    "# 4. Data Generation with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ad1cc",
   "metadata": {},
   "source": [
    "# 5. Optimal Technique Application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bse-nlp-YMIcxN07-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
