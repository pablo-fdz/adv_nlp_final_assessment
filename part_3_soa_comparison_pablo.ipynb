{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Instructions and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Instructions. Part 3: State of the Art Comparison (2 points)\n",
    "\n",
    "- **Objective:** Benchmark your model against the SOA with the full dataset now available.\n",
    "\n",
    "- **Tasks:**\n",
    "  - **a. Full Dataset Training (0.25 points):** Incrementally train your model with varying percentages of the full dataset (1%, 10%, 25%, 50%, 75%, and 100%). Record the results.\n",
    "  - **b. Learning Curve (0.25 points):** Plot a learning curve based on the training data percentages.\n",
    "  - **c. Technique Comparison (0.5 points):** Incorporate the techniques tested in Part 2 into your training schema for comparison.\n",
    "  - **d. Methodology Analysis (1 point):** Analyze and compare all methods employed. Discuss the effectiveness and limitations observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SpaCy model 'fr_core_news_sm' loaded successfully\n",
      "Downloading WordNet...\n",
      "Downloading Open Multilingual WordNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pablo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/pablo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "2025-06-15 20:23:00.848563: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-15 20:23:00.857511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750011780.867454  458834 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750011780.870416  458834 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750011780.878959  458834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750011780.878974  458834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750011780.878975  458834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750011780.878976  458834 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-15 20:23:00.882681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from library.utilities import set_seed, sample_balanced_dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from library.incremental_train.doc import run_incremental_training, train_with_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # specific for mac users with M1 chip (That do not have CUDA)\n",
    "# # !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "# import torch\n",
    "# print(torch.backends.mps.is_available())      # Is Metal available?\n",
    "# print(torch.backends.mps.is_built()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # detect if MPS (GPU in Mac) is available\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# x = torch.ones(3, 3).to(device)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42. This ensures reproducibility of results across runs.\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Full dataset training: Incrementally train your mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded DataFrame shape: (31094, 9)\n",
      "\n",
      "Loaded DataFrame schema:\n",
      "Schema({'id': Int32, 'year': Int32, 'text': String, 'labels': Int64, 'language': String, 'region': String, 'canton': String, 'legal area': String, 'split': String})\n",
      "\n",
      "First few rows of the loaded DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>year</th><th>text</th><th>labels</th><th>language</th><th>region</th><th>canton</th><th>legal area</th><th>split</th></tr><tr><td>i32</td><td>i32</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>2000</td><td>&quot;A.- Par contrat d&#x27;entreprise s…</td><td>0</td><td>&quot;fr&quot;</td><td>null</td><td>null</td><td>&quot;civil law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>1</td><td>2000</td><td>&quot;A.- Le 12 avril 1995, A._ a su…</td><td>0</td><td>&quot;fr&quot;</td><td>null</td><td>null</td><td>&quot;insurance law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>2</td><td>2000</td><td>&quot;A.- En février 1994, M._ a été…</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Région lémanique&quot;</td><td>&quot;ge&quot;</td><td>&quot;insurance law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>3</td><td>2000</td><td>&quot;A.- M._ a travaillé en qualité…</td><td>0</td><td>&quot;fr&quot;</td><td>null</td><td>null</td><td>&quot;insurance law&quot;</td><td>&quot;train&quot;</td></tr><tr><td>6</td><td>2000</td><td>&quot;A.- Le 29 septembre 1997, X._ …</td><td>0</td><td>&quot;fr&quot;</td><td>&quot;Espace Mittelland&quot;</td><td>&quot;ne&quot;</td><td>&quot;penal law&quot;</td><td>&quot;train&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 9)\n",
       "┌─────┬──────┬───────────────────┬────────┬───┬───────────────────┬────────┬───────────────┬───────┐\n",
       "│ id  ┆ year ┆ text              ┆ labels ┆ … ┆ region            ┆ canton ┆ legal area    ┆ split │\n",
       "│ --- ┆ ---  ┆ ---               ┆ ---    ┆   ┆ ---               ┆ ---    ┆ ---           ┆ ---   │\n",
       "│ i32 ┆ i32  ┆ str               ┆ i64    ┆   ┆ str               ┆ str    ┆ str           ┆ str   │\n",
       "╞═════╪══════╪═══════════════════╪════════╪═══╪═══════════════════╪════════╪═══════════════╪═══════╡\n",
       "│ 0   ┆ 2000 ┆ A.- Par contrat   ┆ 0      ┆ … ┆ null              ┆ null   ┆ civil law     ┆ train │\n",
       "│     ┆      ┆ d'entreprise s…   ┆        ┆   ┆                   ┆        ┆               ┆       │\n",
       "│ 1   ┆ 2000 ┆ A.- Le 12 avril   ┆ 0      ┆ … ┆ null              ┆ null   ┆ insurance law ┆ train │\n",
       "│     ┆      ┆ 1995, A._ a su…   ┆        ┆   ┆                   ┆        ┆               ┆       │\n",
       "│ 2   ┆ 2000 ┆ A.- En février    ┆ 0      ┆ … ┆ Région lémanique  ┆ ge     ┆ insurance law ┆ train │\n",
       "│     ┆      ┆ 1994, M._ a été…  ┆        ┆   ┆                   ┆        ┆               ┆       │\n",
       "│ 3   ┆ 2000 ┆ A.- M._ a         ┆ 0      ┆ … ┆ null              ┆ null   ┆ insurance law ┆ train │\n",
       "│     ┆      ┆ travaillé en      ┆        ┆   ┆                   ┆        ┆               ┆       │\n",
       "│     ┆      ┆ qualité…          ┆        ┆   ┆                   ┆        ┆               ┆       │\n",
       "│ 6   ┆ 2000 ┆ A.- Le 29         ┆ 0      ┆ … ┆ Espace Mittelland ┆ ne     ┆ penal law     ┆ train │\n",
       "│     ┆      ┆ septembre 1997,   ┆        ┆   ┆                   ┆        ┆               ┆       │\n",
       "│     ┆      ┆ X._ …             ┆        ┆   ┆                   ┆        ┆               ┆       │\n",
       "└─────┴──────┴───────────────────┴────────┴───┴───────────────────┴────────┴───────────────┴───────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned Parquet file\n",
    "df = pl.read_parquet('data/FRENCH_swiss_judgment_prediction_combined.parquet')\n",
    "\n",
    "# Display the loaded DataFrame\n",
    "print(\"\\nLoaded DataFrame shape:\", df.shape)\n",
    "print(\"\\nLoaded DataFrame schema:\")\n",
    "print(df.schema)\n",
    "print(\"\\nFirst few rows of the loaded DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into training, validation and test sets\n",
    "train_df = df.filter(pl.col('split') == 'train')\n",
    "val_df = df.filter(pl.col('split') == 'validation')\n",
    "test_df = df.filter(pl.col('split') == 'test')\n",
    "\n",
    "# Delete the original data to free up memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the train, validation and test Parquet files\n",
    "# train_df = pl.read_parquet('data/FRENCH_swiss_judgment_prediction_train.parquet')\n",
    "# valid_df = pl.read_parquet('data/FRENCH_swiss_judgment_prediction_valid.parquet')\n",
    "# test_df = pl.read_parquet('data/FRENCH_swiss_judgment_prediction_test.parquet')\n",
    "\n",
    "# # Display the loaded DataFrames\n",
    "# print(\"\\nTrain DataFrame shape:\", train_df.shape)\n",
    "# print(\"\\nValidation DataFrame shape:\", valid_df.shape)\n",
    "# print(\"\\nTest DataFrame shape:\", test_df.shape)\n",
    "\n",
    "# print(\"\\nTrain DataFrame schema:\")\n",
    "# print(train_df.schema)\n",
    "# print(\"\\nFirst few rows of the train DataFrame:\")\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: almanach/camembert-base, Max Length: 512\n"
     ]
    }
   ],
   "source": [
    "model_name = \"almanach/camembert-base\"  # Path to the pre-trained model\n",
    "num_labels = 2  # Number of labels for the classification task (in this case, binary classification)\n",
    "max_length = min(int(AutoModel.from_pretrained(model_name).config.max_position_embeddings), 512)  # Maximum length of the input sequences (truncation if larger than this). Set dynamically based on the chosen model.\n",
    "\n",
    "print(f\"Model: {model_name}, Max Length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = train_with_percentage(train_df, val_df, 1, model_name, max_length, num_labels, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result10 = train_with_percentage(train_df, val_df, 10, model_name, max_length, num_labels, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result25 = train_with_percentage(train_df, val_df, 25, model_name, max_length, num_labels, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sampling with replacement for positive class (need 5294, have 5197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e087b38806d147d9ac0d79f7cdf9e092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd76980a46b42858c26ad1581750d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at almanach/camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='178' max='6620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 178/6620 01:54 < 1:09:58, 1.53 it/s, Epoch 0.53/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Note that, due to the class imbalance, we will have to do sampling with replacement\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# to get enough samples for the minority class\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result50 = \u001b[43mtrain_with_percentage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/pablo/Shared files/GDrive personal/BSE - DSDM/3-Advanced_Methods_in_NLP/Final assessment/adv_nlp_final_assessment/library/incremental_train/doc.py:107\u001b[39m, in \u001b[36mtrain_with_percentage\u001b[39m\u001b[34m(train_df, valid_df, percentage, model_name, max_length, num_labels, seed)\u001b[39m\n\u001b[32m     75\u001b[39m train_args = TrainingArguments(\n\u001b[32m     76\u001b[39m     output_dir=os.path.join(\u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpart_3\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcls_fine_tuning_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msan_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mpct\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     77\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m     gradient_accumulation_steps=\u001b[32m2\u001b[39m\n\u001b[32m     93\u001b[39m )\n\u001b[32m     94\u001b[39m trainer = Trainer(\n\u001b[32m     95\u001b[39m     model=model,\n\u001b[32m     96\u001b[39m     args=train_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m     callbacks=[EarlyStoppingCallback(early_stopping_patience=\u001b[32m5\u001b[39m)]\n\u001b[32m    106\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m train_output = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m log_history = trainer.state.log_history\n\u001b[32m    109\u001b[39m epoch_logs = [log \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m log_history \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2555\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Note that, due to the class imbalance, we will have to do sampling with replacement\n",
    "# to get enough samples for the minority class\n",
    "result50 = train_with_percentage(train_df, val_df, 50, model_name, max_length, num_labels, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51827897ec184a87bab70090bed58bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e72809809df4d37ae057edb7cdfd134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at almanach/camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 42/140 02:19 < 05:42, 0.29 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673912</td>\n",
       "      <td>0.790630</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.708863</td>\n",
       "      <td>0.224233</td>\n",
       "      <td>0.349499</td>\n",
       "      <td>0.212032</td>\n",
       "      <td>0.993837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692725</td>\n",
       "      <td>0.518578</td>\n",
       "      <td>0.317766</td>\n",
       "      <td>0.226059</td>\n",
       "      <td>0.534669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.711588</td>\n",
       "      <td>0.294023</td>\n",
       "      <td>0.347956</td>\n",
       "      <td>0.215766</td>\n",
       "      <td>0.898305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.716725</td>\n",
       "      <td>0.307270</td>\n",
       "      <td>0.350303</td>\n",
       "      <td>0.218031</td>\n",
       "      <td>0.890601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691893</td>\n",
       "      <td>0.525363</td>\n",
       "      <td>0.323353</td>\n",
       "      <td>0.230618</td>\n",
       "      <td>0.540832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that, due to the class imbalance, we will have to do sampling with replacement\n",
    "# to get enough samples for the minority class\n",
    "result75 = train_with_percentage(train_df, val_df, 75, model_name, max_length, num_labels, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.cache/pypoetry/virtualenvs/bse-nlp-YMIcxN07-py3.12/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f157c4adc854416bda5072d0ed48d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/210 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603e42f540394d23bfb3713da8d53ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at almanach/camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 42/140 02:17 < 05:37, 0.29 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673882</td>\n",
       "      <td>0.790630</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.708497</td>\n",
       "      <td>0.227464</td>\n",
       "      <td>0.349742</td>\n",
       "      <td>0.212351</td>\n",
       "      <td>0.990755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692461</td>\n",
       "      <td>0.519548</td>\n",
       "      <td>0.317577</td>\n",
       "      <td>0.226144</td>\n",
       "      <td>0.533128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.710007</td>\n",
       "      <td>0.304362</td>\n",
       "      <td>0.346586</td>\n",
       "      <td>0.215797</td>\n",
       "      <td>0.879815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.719361</td>\n",
       "      <td>0.289176</td>\n",
       "      <td>0.349112</td>\n",
       "      <td>0.216038</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.695721</td>\n",
       "      <td>0.503393</td>\n",
       "      <td>0.327352</td>\n",
       "      <td>0.228606</td>\n",
       "      <td>0.576271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that, due to the class imbalance, we will have to do sampling with replacement\n",
    "# to get enough samples for the minority class\n",
    "result100 = train_with_percentage(train_df, val_df, 100, model_name, max_length, num_labels, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(summary_df, metric='eval_accuracy'):\n",
    "    \"\"\"\n",
    "    Plots a learning curve for the given metric from the summary DataFrame.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(summary_df['percentage'], summary_df[metric], marker='o', label=metric)\n",
    "    plt.xlabel('Percentage of Training Data Used')\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.title(f'Learning Curve: {metric.replace(\"_\", \" \").title()} vs. Training Set Size')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(summary_df['percentage'])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy learning curve\n",
    "plot_learning_curve(summary_df, metric='eval_accuracy')\n",
    "\n",
    "# You can also plot F1, loss, etc.\n",
    "plot_learning_curve(summary_df, metric='eval_f1')\n",
    "plot_learning_curve(summary_df, metric='eval_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Technique Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from library.incremental_train.doc import run_incremental_training\n",
    "\n",
    "# Model names\n",
    "model_names = [\n",
    "    \"almanach/camembert-base\",\n",
    "    \"dascim/juribert-base\",\n",
    "    \"google-bert/bert-base-multilingual-cased\"\n",
    "]\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Train each model \n",
    "for model_name in model_names:\n",
    "    print(f\"\\nEntrenando modelo: {model_name}\")\n",
    "    summary_df = run_incremental_training(\n",
    "        train_df=train_df,\n",
    "        valid_df=val_df,\n",
    "        model_name=model_name,\n",
    "        max_length=max_length,\n",
    "        num_labels=num_labels,\n",
    "        seed=seed\n",
    "    )\n",
    "    results[model_name] = summary_df\n",
    "\n",
    "# Graph the learning curves\n",
    "def plot_comparison(results, metric='eval_accuracy'):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for model_name, summary_df in results.items():\n",
    "        plt.plot(summary_df['percentage'], summary_df[metric], marker='o', label=model_name.split('/')[-1])\n",
    "    plt.xlabel('Percentage of Training Data Used')\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.title(f'Learning Curve Comparison: {metric.replace(\"_\", \" \").title()}')\n",
    "    plt.grid(True)\n",
    "    plt.xticks([1, 10, 25, 50, 75, 100])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Comparison of accuracy and F1 scores\n",
    "plot_comparison(results, metric='eval_accuracy')\n",
    "plot_comparison(results, metric='eval_f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Methodology Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bse-nlp-YMIcxN07-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
